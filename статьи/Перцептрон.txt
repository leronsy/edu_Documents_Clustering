 Логическая схема перцептрона с тремя выходами

Перцептрон или персептронВариант перцептрон - изначальный используется в переводе книги Розенблатта 1965 также в справочнике  Вариант персептрон встречается чаще он возник при переводе книги Минского и Пейперта 1971 см также   от  - математическая или компьютерная модель восприятия информации кибернетическая Фрэнком Розенблаттом в Марк-1Марк-1 в частности был системой имитирующей человеческий глаз и его взаимодействие с мозгом в нейросетей а Марк-1 - первым в мире ассоциативным элементам а затем реагирующим элементам Таким образом перцептроны позволяют создать набор ассоциаций между входными реакцией на выходе В биологическом плане это соответствует преобразованию например зрительной информации в физиологический ответ от двигательных нейронов Согласно современной терминологии перцептроны могут быть классифицированы как искусственные нейронные сети
 с одним скрытым слоемТрёхслойные по классификации принятой у Розенблатта и двухслойные по современной системе обозначений - с той особенностью что первый слой не обучаемый
 с пороговой передаточной функцией
 с прямым распространением сигнала

На фоне роста популярности нейронных сетей в Марвина Минского и Сеймура Паперта которая показала принципиальные ограничения перцептронов Это привело к смещению интереса исследователей искусственного интеллекта в противоположную от нейросетей область символьных вычисленийК символьному подходу относятся например создание экспертных систем организация баз знаний анализ текстов Кроме того из-за сложности математического исследования перцептронов а также отсутствия общепринятой терминологии возникли различные неточности и заблуждения

Впоследствии интерес к нейросетям и в частности работам Розенблатта возобновился Так например сейчас стремительно развивается бактериородопсин-содержащих пленок

 Появление перцептрона 

искусственного нейрона - базового элемента любой нейронной сети

В Уоррен Мак-Каллок и Уолтер Питтс предложили понятие искусственной нейронной сети В частности ими была предложена модель искусственного нейрона Дональд Хебб в работе Организация поведения Современное издание  Фрэнк Розенблатт Он предложил схему устройства моделирующего процесс человеческого восприятия и назвал его перцептроном Перцептрон передавал сигналы от Корнеллском университете был продемонстрирован первый Фрэнк Розенблатт со своим творением - Марк-1
Чтобы научить перцептрон классифицировать образы был разработан специальный итерационный метод обучения проб и ошибок напоминающий процесс обучения человека - обобщать буквы написанные различным образом почерком в один обобщённый образ Однако возможности перцептрона были ограниченными машина не могла надёжно распознавать частично закрытые буквы а также буквы иного размера расположенные со сдвигом или поворотом нежели те которые использовались на этапе её обученияМинский М Пейперт С с 50

Отчёт по первым результатам появился ещё в многослойных перцептронов с перекрёстными третья глава и обратными четвёртая глава связями В книге также вводится ряд важных идей и теорем например доказывается Поступление сигналов с сенсорного поля в решающие блоки элементарного перцептрона в его физическом воплощении
Логическая схема элементарного перцептрона Веса S-A связей могут иметь значения 1 1 или 0 то есть отсутствие связи Веса A-R связей W могут быть любыми
Элементарный перцептрон состоит из элементов трёх типов S-элементов A-элементов и одного R-элемента S-элементы - это слой сенсоров или рецепторов В физическом воплощении они соответствуют например светочувствительным клеткам сетчатки глаза или одиночные нейроны Таким образом если набор соответствующих S-элементов располагается на сенсорном поле в форме буквы Д A-элемент активизируется если достаточное количество рецепторов сообщило о появлении белого пятна света в их окрестности то есть A-элемент будет как бы ассоциирован с наличиемотсутствием буквы Д в некоторой области

Сигналы от возбудившихся A-элементов в свою очередь передаются в сумматор R причём сигнал от i-го ассоциативного элемента передаётся с коэффициентом w_Фомин С В Беркинблит М Б  Математические проблемы в биологии Этот коэффициент называется весом A-R связи

Так же как и A-элементы R-элемент подсчитывает сумму значений входных сигналов помноженных на веса линейную форму R-элемент а вместе с ним и элементарный перцептрон выдаёт 1 если линейная форма превышает порог θ иначе на выходе будет 1 Математически функцию реализуемую R-элементом можно записать так
 fx  operatornamesum_ w_i x_i - theta

Обучение элементарного перцептрона состоит в изменении весовых коэффициентов w_i связей A-R Веса связей S-A которые могут принимать значения  и значения порогов A-элементов выбираются случайным образом в самом начале и затем не изменяются Описание алгоритма см ниже

После обучения перцептрон готов работать в режиме распознаванияРозенблатт Ф с 158-162 или обобщенияРозенблатт Ф с 162-163 В этом режиме перцептрону предъявляются ранее неизвестные ему объекты и перцептрон должен установить к какому классу они принадлежат Работа перцептрона состоит в следующем при предъявлении объекта возбудившиеся A-элементы передают сигнал R-элементу равный сумме соответствующих коэффициентов w_i Если эта сумма положительна то принимается решение что данный объект принадлежит к первому классу а если она отрицательна - то ко второмуБрюхомицкий Ю А Нейросетевые модели для систем информационной безопасности 2005

 Основные понятия теории перцептронов 
Серьёзное ознакомление с теорией перцептронов требует знания базовых определений и теорем совокупность которых и представляет собой базовую основу для всех последующих видов искусственных нейронных сетей Но как минимум необходимо понимание хотя бы с точки зрения теории сигналов являющееся оригинальным то есть описанное автором перцептрона Ф Розенблаттом

 Описание на основе сигналов 
Пороговая функция реализуемая простыми S- и A-элементами
Пороговая функция реализуемая простым R-элементом
Для начала определим составные элементы перцептрона которые являются частными случаями искусственного нейрона с пороговой передаточной функцией
 Простым S-элементом сенсорным является чувствительный элемент который от воздействия какого-либо из видов энергии например света звука давления тепла и т п вырабатывает сигнал Если входной сигнал превышает некоторый порог θ на выходе элемента получаем 1 в противном случае - 0Розенблатт Ф с 81
 Простым A-элементом ассоциативным называется логический решающий элемент который даёт выходной сигнал 1 когда линейнаяРозенблатт Ф с 200

В результате Розенблатт ввёл следующие определения
 Перцептрон представляет собой сеть состоящую из S- A- R-элементов с переменной матрицей взаимодействия W элементы которой w_ - весовые коэффициенты определяемой последовательностью прошлых состояний активности сетиРозенблатт Ф с 82
 Перцептроном с последовательными связями называется система в которой все связи начинающиеся от элементов с логическим расстоянием d от ближайшего S-элемента оканчиваются на элементах с логическим расстоянием d1 от ближайшего S-элемента
 Простым перцептроном называется любая система удовлетворяющая следующим пяти условиям
 в системе имеется только один R-элемент естественно он связан всеми A-элементами
 система представляет собой перцептрон с последовательными связями идущими только от S-элементов к A-элементам и от A-элементов к R-элементам
 веса всех связей от S-элементов к A-элементам S-A связей неизменны
 время передачи каждой связи равно либо нулю либо фиксированной постоянной tau
 все активирующие функции S- A- R-элементов имеют вид U_t  fa_t где a_t - алгебраическая сумма всех сигналов поступающих одновременно на вход элемента u_Розенблатт Ф с 83
 Элементарным перцептроном называется простой перцептрон у которого все элементы - простые В этом случае его активизирующая функция имеет вид c_t  U_t - tauw_tРозенблатт Ф с 93

Дополнительно можно указать на следующие концепции предложенные в книге и позднее развитые в рамках теории нейронных сетей
 рекуррентными
 параллельных вычислений частным случаем которых на то время был перцептрон Для анализа его свойств ему пришлось переизложить теорию перцептронов на язык Архитектура многослойного перцептрона обоих подтипов
Понятие перцептрона имеет интересную но незавидную историю В результате неразвитой терминологии нейронных сетей прошлых лет резкой критики и непонимания задач исследования перцептронов а иногда и ложного освещения прессой изначальный смысл этого понятия исказился Сравнивая разработки Розенблатта и современные обзоры и статьи можно выделить 4 довольно обособленных класса перцептронов

 Перцептрон с одним скрытым слоем
 Это классический перцептрон которому посвящена большая часть книги Розенблатта и рассматриваемый в данной статье у него имеется по одному слою S- A- и R-элементов

 Однослойный перцептрон
 Это модель в которой входные элементы напрямую соединены с выходными с помощью системы весов Является простейшей сетью прямого распространения - линейным классификатором и частным случаем классического перцептрона в котором каждый S-элемент однозначно соответствует одному A-элементу S-A связи имеют вес 1 и все A-элементы имеют порог θ  1 Однослойные перцептроны фактически являются формальными нейронами то есть пороговыми элементами Мак-Каллока - Питтса Они имеют множество ограничений в частности они не могут идентифицировать ситуацию когда на их входы поданы разные сигналы задача XOR см ниже

 Многослойный перцептрон по Розенблатту
 Это перцептрон в котором присутствуют дополнительные слои A-элементов Его анализ провёл Розенблатт в третьей части своей книги

 Многослойный перцептрон по Румельхарту
 Это перцептрон в котором присутствуют дополнительные слои A-элементов причём обучение такой сети проводится по методу обратного распространения ошибки и обучаемыми являются все слои перцептрона в том числе S-A Является частным случаем многослойного перцептрона Розенблатта

В настоящее время в литературе под термином перцептрон понимается чаще всего однослойный перцептрон  причём существует распространённое заблуждение что именно этот простейший тип моделей предложил Розенблатт В противоположность однослойному ставят многослойный перцептрон  опять же чаще всего подразумевая многослойный перцептрон Румельхарта а не Розенблатта Классический перцептрон в такой дихотомии относят к многослойным

 Алгоритмы обучения 
Важным свойством любой нейронной сети является способность к обучению Процесс обучения является процедурой настройки весов и порогов с целью уменьшения разности между желаемыми целевыми и получаемыми векторами на выходе В своей книге Розенблатт пытался классифицировать различные алгоритмы обучения перцептрона называя их системами подкрепления
 Система подкрепления - это любой набор правил на основании которых можно изменять с течением времени матрицу взаимодействия или состояние памяти перцептронаРозенблатт Ф с 85-88
Описывая эти системы подкрепления и уточняя возможные их виды Розенблатт основывался на идеях Д Хебба об обучении предложенных им в обучения с учителем при котором вес связи не изменяется до тех пор пока текущая реакция перцептрона остаётся правильной При появлении неправильной реакции вес изменяется на единицу а знак - определяется противоположным от знака ошибки

Допустим мы хотим обучить перцептрон разделять два класса объектов так чтобы при предъявлении объектов первого класса выход перцептрона был положителен 1 а при предъявлении объектов второго класса - отрицательным 1 Для этого выполним следующий алгоритм
 Случайным образом выбираем пороги для A-элементов и устанавливаем связи S-A далее они изменяться не будут
 Начальные коэффициенты w_i полагаем равными нулю
 Предъявляем обучающую выборку объекты например круги либо квадраты с указанием класса к которым они принадлежат
 Показываем перцептрону объект первого класса При этом некоторые A-элементы возбудятся Коэффициенты w_i соответствующие этим возбуждённым элементам увеличиваем на 1
 Предъявляем объект второго класса и коэффициенты w_i тех A-элементов которые возбудятся при этом показе уменьшаем на 1
 Обе части шага 3 выполним для всей обучающей выборки В результате обучения сформируются значения весов связей w_i

обучении без учителя предложив следующий способ обучения
 Альфа-система подкрепления - это система подкрепления при которой веса всех активных связей c_ которые ведут к элементу u_j изменяются на одинаковую величину r а веса неактивных связей за это время не изменяютсяРозенблатт Ф с 86
Затем с разработкой понятия многослойного перцептрона альфа-система была модифицирована и её стали называть дифференцируемой например градиентного спуска благодаря которому возможно обучение более одного слоя

 Метод обратного распространения ошибки 

Для обучения многослойных сетей рядом учёных в том числе Д Румельхартом был предложен градиентный алгоритм обучения с учителем проводящий сигнал ошибки вычисленный выходами перцептрона к его входам слой за слоем Сейчас это самый популярный метод обучения многослойных перцептронов Его преимущество в том что он может обучить все слои нейронной сети и его легко просчитать локально Однако этот метод является очень долгим к тому же для его применения нужно чтобы передаточная функция нейронов была дифференцируемой При этом в перцептронах пришлось отказаться от бинарного сигнала и пользоваться на входе непрерывными значениямиХайкин С 2006 с 225-243 304-316

 Традиционные заблуждения 
В результате популяризации искусственных нейронных сетей журналистами и маркетологами был допущен ряд неточностей которые при недостаточном изучении оригинальных работ по этой тематике неверно истолковывались молодыми на то время учёными В результате по сей день можно встретиться с недостаточно глубокой трактовкой функциональных возможностей перцептрона по сравнению с другими нейронными сетями разработанными в последующие годы

 Терминологические неточности 
Самая распространённая ошибка связанная с терминологией это определение перцептрона как нейронной сети без скрытых слоёв однослойного перцептрона см выше Эта ошибка связана с недостаточно проработанной терминологией в области нейросетей на раннем этапе их разработки Ф Уоссерменом была сделана попытка определённым образом классифицировать различные виды нейронных сетей

Как видно из публикаций нет общепринятого способа подсчёта числа слоёв в сети Многослойная сеть состоит из чередующихся множеств нейронов и весов Входной слой не выполняет суммирования Эти нейроны служат лишь в качестве разветвлений для первого множества весов и не влияют на вычислительные возможности сети По этой причине первый слой не принимается во внимание при подсчёте слоев и сеть считается двухслойной так как только два слоя выполняют вычисления Далее веса слоя считаются связанными со следующими за ними нейронами Следовательно слой состоит из множества весов со следующими за ними нейронами суммирующими взвешенные сигналыУоссермен Ф Нейрокомпьютерная техника Теория и практика 1992

В результате такого представления перцептрон попал под определение однослойная нейронная сеть Отчасти это верно потому что у него нет скрытых слоёв обучающихся нейронов веса которых адаптируются к задаче И поэтому всю совокупность фиксированных связей системы из S- к A-элементам можно логически заменить набором модифицированных по жёсткому правилу новых входных сигналов поступающих сразу на А-элементы устранив тем самым вообще первый слой связей Но тут как раз не учитывают что такая модификация превращает нелинейное представление задачи в линейное

Поэтому просто игнорирование не обучаемых слоёв с фиксированными связями в элементарном перцептроне это S-A связи позволяет делать неправильные выводы о возможностях нейросети Так Минский поступил очень корректно переформулировав А-элемент как рецептивного поля S-элементов на ассоциативное поле А-элементов в результате чего и происходит преобразование любой линейно неразделимой задачи в линейно разделимую

 Функциональные заблуждения 
Решение элементарным перцептроном задачи XOR Порог всех элементов θ  0
Большинство функциональных заблуждений сводятся к якобы невозможности решения перцептроном нелинейно разделяемой задачи Но вариаций на эту тему достаточно много рассмотрим главные из них

 Задача XOR 
Перцептрон не способен решить задачу XOR
 Очень распространённое и самое несерьёзное заявление На изображении справа показано решение этой задачи перцептроном Данное заблуждение возникает во-первых из-за того что неправильно интерпретируют определение перцептрона данного Минским см выше а именно предикаты сразу приравнивают входам хотя предикат у Минского - это функция идентифицирующая целый набор входных значенийПредикат эквивалентен входу лишь в частном случае - только когда он зависит от одного аргумента Во-вторых из-за того что классический перцептрон Розенблатта путают с однослойным перцептроном из-за терминологической неточности описанной выше
Следует обратить особое внимание на то что однослойный перцептрон в современной терминологии и однослойный перцептрон в терминологии Уоссермана - разные объекты И объект изображённый на иллюстрации в терминологии Уоссермана есть двухслойный перцептрон

 Обучаемость линейно неразделимым задачам 
Выбором случайных весов можно достигнуть обучения и линейно неразделимым вообще любым задачам но только если повезёт и в новых переменных выходах A-нейронов задача окажется линейно разделимой Но может и не повезти

 G-матрице - вероятность решения равна 100 То есть при отображении рецепторного поля на ассоциативное поле большей на одну размерности случайным нелинейным оператором нелинейная задача превращается в линейно разделимую А следующий обучаемый слой уже находит линейное решение в другом пространстве входов

 Например обучение перцептрона для решения задачи XOR см на иллюстрации проводится следующими этапами



 Обучаемость на малом числе примеров 
Если в задаче размерность входов довольно высока а обучающих примеров мало то в таком слабо заполненном пространстве число удач может и не оказаться малым Это свидетельствует лишь о частном случае пригодности перцептрона а не его универсальности

 Данный аргумент легко проверить на тестовой задаче под названием шахматная доска или губка с водойБонгард М М с 29М М Бонгард считает эту задачу наисложнейшей для проведения гиперплоскости в пространстве рецепторов
ЗадачаДана цепочка из 2N единиц или нулей параллельно поступающих на входы перцептрона Если эта цепочка является зеркально симметричной относительно центра то на выходе 1 иначе 0 Обучающие примеры - все это важно 2 цепочек
 Могут быть вариации данной задачи например

 Если данный аргумент справедлив то перцептрон не сможет ни при каких условиях обучиться не делая ни одной ошибки Иначе перцептрон не ошибётся ни разу

 На практике оказывается что данная задача очень проста для перцептрона чтобы её решить перцептрону достаточно 1500 А-элементов вместо полных 65 536 необходимых для любой задачи При этом число итераций порядка 1000 При 1000 А-элементов перцептрон не сходится за 10 000 итераций Если же увеличить число А-элементов до 40 000 то схождения можно ожидать за 30-80 итераций

 Такой аргумент появляется из-за того что данную задачу путают с задачей Минского о предикате чётностьМинский М Пейперт С с 59

 Стабилизация весов и сходимость 
В перцептроне Розенблатта столько А-элементов сколько входов И сходимость по Розенблатту это стабилизация весов

 У Розенблатта читаем

Если число стимулов в пространстве W равно n  N то есть больше числа А-элементов элементарного перцептрона то существует некоторая теорему сходимости перцептрона Таким образом при возрастании числа рецепторов если количество А-элементов фиксировано непосредственно не зависит возможность перцептрона к решению задач произвольной сложности

 Такое заблуждение происходит от следующей фразы Минского

При исследовании предиката чётность мы видели что коэффициенты могут расти с ростом R числа точек на изображении экспоненциальноМинский Пейперт с 155 189 недословно упрощённо для выразительности

 Кроме того Минский исследовал и другие предикаты например равенство Но все эти предикаты представляют собой достаточно специфическую задачу на обобщение а не на распознавание или прогнозирование Так например чтобы перцептрон мог выполнять предикат четность - он должен сказать чётно или нет число чёрных точек на чёрно-белом изображении а для выполнения предиката равенство - сказать равна ли правая часть изображения левой Ясно что такие задачи выходят за рамки задач распознавания и прогнозирования и представляют собой задачи на обобщение или просто на подсчёт определённых характеристик Это и было убедительно показано Минским и является ограничением не только перцептронов но и всех параллельных алгоритмов которые не способны быстрее последовательных алгоритмов вычислить такие предикаты

 Поэтому такие задачи ограничивают возможности всех нейронных сетей и перцептронов в частности но это никак не связанно с фиксированными связями первого слоя так как во-первых речь шла о величине коэффициентов связей второго слоя а во-вторых вопрос только в эффективности а не принципиальной возможности То есть перцептрон можно обучить и этой задаче но требуемые для этого ёмкость памяти и скорость обучения будут больше чем при применении простого последовательного алгоритма Введение же обучаемых весовых коэффициентов в первом слое лишь ухудшит положение дел ибо потребует большего времени обучения потому что переменные связи между S и A скорее препятствуют чем способствуют процессу обученияРозенблатт стр 239 Причём при подготовке перцептрона к задаче распознавания стимулов особого типа для сохранения эффективности потребуются особые условия стохастического обученияРозенблатт стр 242 что было показано Розенблаттом в экспериментах с перцептроном с переменными S-A связями

 Возможности и ограничения модели 


 Возможности модели 
классификации объектов Зелёная линия - граница классов
Сам Розенблатт рассматривал перцептрон прежде всего как следующий важный шаг в сторону исследования и использования нейронных сетей а не как оконченный вариант машины способной мыслитьНа первых этапах развития науки об искусственном интеллекте её задача рассматривалась в абстрактном смысле - создание систем напоминающих по разуму человека см психологических тестов для определения возможностей нейросетей эксперименты по различению обобщению по распознаванию последовательностей образованию абстрактных понятий формированию и свойствам самосознания творческого воображения и другиеРозенблатт Ф с 70-77 Некоторые из этих экспериментов далеки от современных возможностей перцептронов поэтому их развитие происходит больше философски в пределах направления классификации объектов и возможность аппроксимации границ классов и функцийсм Ежов А А Шумский С А Нейрокомпьютинг 2006  Лекция 3 Обучение с учителем Распознавание образов

Важным свойством перцептронов является их способность к обучению причём по довольно простому и эффективному алгоритму см выше

 Ограничения модели 



Некоторые задачи которые перцептрон не способен решить 1 2 - преобразования группы переносов 3 - из какого количества частей состоит фигура 4 - внутри какого объекта нет другой фигуры 5 - какая фигура внутри объектов повторяется два раза 3 4 5 - задачи на определение связности фигур

Сам Розенблатт выделил два фундаментальных ограничения для трёхслойных перцептронов состоящих из одного S-слоя одного A-слоя и R-слоя отсутствие у них способности к обобщению своих характеристик на новые стимулы или новые ситуации а также неспособность анализировать сложные ситуации во внешней среде путём расчленения их на более простые

В Марвин Минский и Сеймур Паперт опубликовали книгу Перцептроны где математически показали что перцептроны подобные розенблаттовским принципиально не в состоянии выполнять многие из тех функций которые хотели получить от перцептронов К тому же в то время была слабо развита теория о параллельных вычислениях а перцептрон полностью соответствовал принципам таких вычислений По большому счёту Минский показал преимущество последовательных вычислений перед параллельным в определённых классах задач связанных с инвариантным представлением Его критику можно разделить на три темы

 Перцептроны имеют ограничения в задачах связанных с инвариантным представлением образов то есть независимым от их положения на сенсорном поле и относительно других фигур Такие задачи возникают например если нам требуется построить машину для чтения печатных букв или цифр так чтобы эта машина могла распознавать их независимо от положения на странице то есть чтобы на решение машины не оказывали влияния перенос аналитическими методами например статистическими в задачах связанных с производительный метод анализа данных
 Было показано что некоторые задачи в принципе могут быть решены перцептроном но могут потребовать нереально большого времениМинский Пейперт с 163-187 или нереально большой памятиМинский Пейперт с 153-162

Книга Минского и Паперта существенно повлияла на пути развития науки об искусственном интеллекте так как переместила научный интерес и субсидии правительственных организаций символьный подход в ИИ

 Применение перцептронов 
Здесь будут показаны только основы практического применения перцептрона на двух различных задачах Задача прогнозирования и эквивалентная ей задача распознавания образов требует высокой точности а задача управления агентами - высокой скорости обучения Поэтому рассматривая эти задачи можно полноценно ознакомиться с возможностями перцептрона однако этим далеко не исчерпываются варианты его использования

В практических задачах от перцептрона потребуется возможность выбора более чем из двух вариантов а значит на выходе у него должно находиться более одного R-элемента Как показано Розенблаттом характеристики таких систем не отличаются существенно от характеристик элементарного перцептронаРозенблатт Ф с 219-224

 Прогнозирование и распознавание образов 
обучающегося агента со средой Важной частью такой системы являются обратные связи
В этих задачах от перцептрона требуется установить принадлежность объекта к какому-либо классу по его параметрам например по внешнему виду форме силуэту Причём точность распознавания будет во многом зависеть от представления выходных реакций перцептрона Здесь возможны три типа кодирования конфигурационное позиционное и гибридное Позиционное кодирование когда каждому классу соответствует свой R-элемент даёт более точные результаты чем другие виды Такой тип использован например в работе Э Куссуль и др Перцептроны Розенблатта для распознавания рукописных цифр Однако оно неприменимо в тех случаях когда число классов значительно например несколько сотен В таких случаях можно применять гибридное конфигурационно-позиционное кодирование как это было сделано в работе С Яковлева Система распознавания движущихся объектов на базе искусственных нейронных сетей

 Управление агентами 
В искусственном интеллекте часто рассматриваются обучающиеся адаптирующиеся к окружающей среде агенты При этом в условиях неопределённости становится важным анализировать не только текущую информацию но и общий контекст ситуации в которую попал агент поэтому здесь применяются рефрактерностиЯковлев С С Investigation of Refractoriness principle in Recurrent Neural Networks Scientific proceedings of Riga Technical University Issue 5 Vol36 RTU Riga 2008 P 41-48  Исследование принципа рефрактерности в рекуррентных нейронных сетях перевод 

После периода известного как 1980-х годах так как сторонники символьного подхода в ИИ так и не смогли подобраться к решению вопросов о Понимании и Значении из-за чего машинный перевод и техническое распознавание образов до сих пор обладает неустранимыми недостатками Сам Минский публично выразил сожаление что его выступление нанесло урон концепции перцептронов хотя книга лишь показывала недостатки отдельно взятого устройства и некоторых его вариаций Но в основном ИИ стал синонимом символьного подхода который выражался в составлении все более сложных программ для компьютеров моделирующих сложную деятельность человеческого мозга

 См также 
 Свёрточная нейронная сеть
 Биокомпьютинг
 Байесовская сеть доверия
 Когнитрон
 История искусственного интеллекта
 Паттерн психология

 Примечания 


 Источники 


 Литература 
 
 
 
 
 
 
 
 
  
  

 Ссылки 