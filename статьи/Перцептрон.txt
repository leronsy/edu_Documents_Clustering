 ФайлPerceptronrusvgthumb300pxЛогическая схема перцептрона с тремя выходами

Перцептрон или персептронref groupnbВариант перцептрон  изначальный используется в переводе книги Розенблатта 1965 также в справочнике книгаавтор  часть  заглавие  Толковый словарь по искусственному интеллектуоригинал  ссылка    Авторысоставители АnbspНnbspАверкин МnbspГnbspГаазеРапопорт Поспелов Д АДnbspАnbspПоспеловиздание  место  Миздательство  Радио и связьгод  1992том  страницы  страниц  256серия  isbn   Вариант персептрон встречается чаще он возник при переводе книги Минского и Пейперта 1971 см также книгаавтор  часть  заглавие  Энциклопедия кибернетики Том 2 МихЯчоригинал  ссылка    издание  место  Киевиздательство  Гл изд УСЭгод  1974том  страницы  156158страниц  серия  isbn  ref langenperceptron от langlaperceptio  восприятие langdePerzeptron  Имитационное моделированиематематическая или компьютерное моделированиекомпьютерная модель Восприятиевосприятия Информацияинформации мозгом кибернетикакибернетическая модель мозга предложенная Розенблатт ФрэнкФрэнком Розенблаттом в 1957 году и впервые реализованная в виде электронной машины Марк1Марк1ref groupnbМарк1 в частности был системой имитирующей Зрительная системачеловеческий глаз и его взаимодействие с мозгомref в 1960 году Перцептрон стал одной из первых моделей Искусственная нейронная сетьнейросетей а Марк1  первым в мире нейрокомпьютером

Перцептрон состоит из трёх типов элементов а именно поступающие от датчиков сигналы передаются ассоциативная памятьассоциативным элементам а затем реагирующим элементам Таким образом перцептроны позволяют создать набор Ассоциация психологияассоциаций помоему тут уж на рефлекс надо ссылаться проще Incnis Mrsi  между входными стимулами и необходимой Реакция биологияреакцией на выходе В биологическом плане это соответствует преобразованию например зрительной информации в Физиологическая реакцияфизиологический ответ от двигательных Нейронынейронов Согласно современной терминологии перцептроны могут быть классифицированы как искусственные нейронные сети
 с одним скрытым слоемref groupnbТрёхслойные по классификации принятой у Розенблатта и двухслойные по современной системе обозначений  с той особенностью что первый слой не обучаемыйref
 с Искусственный нейронПороговая передаточная функцияпороговой передаточной функцией
 с Искусственная нейронная сетьСети прямого распространения Feedforwardпрямым распространением сигнала

На фоне роста популярности нейронных сетей в 1969 году вышла книга Минский МарвинМарвина Минского и Паперт СеймурСеймура Паперта которая показала принципиальные ограничения перцептронов Это привело к смещению интереса исследователей Искусственный интеллектискусственного интеллекта в противоположную от нейросетей область символьные вычислениясимвольных вычисленийref groupnbК символьному подходу относятся например создание Экспертная системаэкспертных систем организация База знанийбаз знаний Обработка естественного языкаанализ текстовref Кроме того изза сложности Математикаматематического исследования перцептронов а также отсутствия общепринятой терминологии возникли различные Традиционные заблуждениянеточности и заблуждения

Впоследствии интерес к нейросетям и в частности работам Розенблатта возобновился Так например сейчас стремительно развивается биокомпьютинг который в своей теоретической основе вычислений в том числе базируется на нейронных сетях а перцептрон воспроизводят на основе Бактериородопсинсодержащие пленкибактериородопсинсодержащих пленок

 Появление перцептрона 

ФайлSingle layer perceptronpngthumbleft180pxСхема Искусственный нейронискусственного нейрона  базового элемента любой нейронной сети

В 1943 году в своей статье Логическое исчисление идей относящихся к нервной активностиrefстатья  автор  langen Warren S McCulloch and Walter Pitts  заглавие  Логическое исчисление идей относящихся к нервной активности
 оригинал   langenA logical calculus of the ideas immanent in nervous activity  ссылка    издание  langenBulletin of Mathematical Biology  место  langenNew York  издательство  langenSpringer New York  год  1943  выпуск   том  5  номер  4  страницы  115133  isbn  ref Маккалок УорренУоррен МакКаллок и Питтс УолтерУолтер Питтс предложили понятие Искусственная нейронная сетьискусственной нейронной сети В частности ими была предложена модель Искусственный нейронискусственного нейрона Хебб ДональдДональд Хебб в работе Организация поведенияref namehebbкнига  автор  langenDonald Olding Hebb  заглавие  langenThe Organization of Behavior A Neuropsychological Theory  издательство  langenWiley  год  1949  allpages  335  Современное издание книга  автор  langenDonald Olding Hebb  заглавие  langenThe Organization of Behavior A Neuropsychological Theory  ссылка    издательство  langenLawrence Erlbaum Associates  год  2002  allpages  335  isbn  0805843000 ISBN 9780805843002 ref 1949 года описал основные принципы обучения нейронов

Эти идеи несколько лет спустя развил американский нейрофизиолог Розенблатт ФрэнкФрэнк Розенблатт Он предложил схему устройства Имитационное моделированиемоделирующего процесс органы чувствчеловеческого восприятия и назвал его перцептроном Перцептрон передавал сигналы от фотоэлементов представляющих собой сенсорное поле в блоки электромеханических ячеек памяти Эти ячейки соединялись между собой случайным образом в соответствии с принципами коннективизма В 1957 году в Корнеллской Лаборатории Аэронавтики было успешно завершено моделирование работы перцептрона на компьютере IBM 704 а два года спустя 23 июня 1960 года в Корнеллский университетКорнеллском университете был продемонстрирован первый нейрокомпьютер  Марк1 который был способен распознавать некоторые буквы английского алфавитаref Perceptrons An Associative Learning Networkrefref Появление перцептронаref

ФайлRosenblattjpgthumbright200pxФрэнк Розенблатт со своим творением  Марк1
Чтобы научить перцептрон классифицировать образы был разработан специальный итерационный метод обучения проб и ошибок напоминающий процесс обучения человека  метод коррекции ошибкиref namelearning Системы распознавания образовref Кроме того при распознании той или иной буквы перцептрон мог выделять характерные особенности буквы статистически чаще встречающиеся чем малозначимые отличия в индивидуальных случаях Тем самым перцептрон был способен Обобщение понятийобобщать буквы написанные различным образом почерком в один обобщённый образ Однако возможности перцептрона были ограниченными машина не могла надёжно распознавать частично закрытые буквы а также буквы иного размера расположенные со сдвигом или поворотом нежели те которые использовались на этапе её обученияref nameinvariantМинский М Пейперт С с 50ref

Отчёт по первым результатам появился ещё в 1958 году  тогда Розенблаттом была опубликована статья Перцептрон Вероятная модель хранения и организации информации в головном мозгеref The Perceptron A Probabilistic Model for Information Storage and Organization in the Brainref Но подробнее свои теории и предположения относительно процессов восприятия и перцептронов он описывает 1962 году в книге Принципы нейродинамики Перцептроны и теория механизмов мозга В книге он рассматривает не только уже готовые модели перцептрона с одним скрытым слоем но и Многослойный перцептрон Розенблаттамногослойных перцептронов с Перцептроны с перекрестными связямиперекрёстными третья глава и Перцептроны с обратной связьюобратными четвёртая глава связями В книге также вводится ряд важных идей и теорем например доказывается теорема сходимости перцептронаref nameteorshodРозенблатт Ф с 102ref

 Описание элементарного перцептрона 
ФайлPerceptron physical implementationrusvg280pxthumbleftПоступление сигналов с сенсорного поля в решающие блоки элементарного перцептрона в его физическом воплощении
ФайлSimple perceptronsvgthumb280pxleftЛогическая схема элементарного перцептрона Веса SA связей могут иметь значения 1 1 или 0 то есть отсутствие связи Веса AR связей W могут быть любыми
Элементарный перцептрон состоит из элементов трёх типов Sэлементов Aэлементов и одного Rэлемента Sэлементы  это слой сенсоров или рецепторов В физическом воплощении они соответствуют например светочувствительным клеткам Сетчаткасетчатки глаза или фоторезисторам матрицы камеры Каждый рецептор может находиться в одном из двух состояний  покоя или возбуждения и только в последнем случае он передаёт единичный сигнал в следующий слой ассоциативным элементам

Aэлементы называются ассоциативными потому что каждому такому элементу как правило соответствует целый набор ассоциация Sэлементов Aэлемент активизируется как только количество сигналов от Sэлементов на его входе превысило некоторую величину θref groupnbФормально Aэлементы как и Rэлементы представляют собой сумматоры с порогом то есть Искусственный нейронодиночные нейроныref Таким образом если набор соответствующих Sэлементов располагается на сенсорном поле в форме буквы Д Aэлемент активизируется если достаточное количество рецепторов сообщило о появлении белого пятна света в их окрестности то есть Aэлемент будет как бы ассоциирован с наличиемотсутствием буквы Д в некоторой области

Сигналы от возбудившихся Aэлементов в свою очередь передаются в сумматор R причём сигнал от iго ассоциативного элемента передаётся с коэффициентом mathw_imathrefФомин С В Беркинблит М Б  Математические проблемы в биологииref Этот коэффициент называется весом AR связи

Так же как и Aэлементы Rэлемент подсчитывает сумму значений входных сигналов помноженных на веса Линейная формалинейную форму Rэлемент а вместе с ним и элементарный перцептрон выдаёт 1 если линейная форма превышает порог θ иначе на выходе будет 1 Математически функцию реализуемую Rэлементом можно записать так
 mathfx  operatornamesignsum_i1n w_i x_i  thetamath

Машинное обучениеОбучение элементарного перцептрона состоит в изменении весовых коэффициентов mathw_imath связей AR Веса связей SA которые могут принимать значения 1 0 1 и значения порогов Aэлементов выбираются случайным образом в самом начале и затем не изменяются Описание алгоритма см Алгоритмы обученияниже

После обучения перцептрон готов работать в режиме распознавание образовраспознаванияrefРозенблатт Ф с 158162ref или обобщение понятийобобщенияrefРозенблатт Ф с 162163ref В этом режиме перцептрону предъявляются ранее неизвестные ему объекты и перцептрон должен установить к какому классу они принадлежат Работа перцептрона состоит в следующем при предъявлении объекта возбудившиеся Aэлементы передают сигнал Rэлементу равный сумме соответствующих коэффициентов mathw_imath Если эта сумма положительна то принимается решение что данный объект принадлежит к первому классу а если она отрицательна  то ко второмуrefБрюхомицкий Ю А Нейросетевые модели для систем информационной безопасности 2005ref

 Основные понятия теории перцептронов 
Серьёзное ознакомление с теорией перцептронов требует знания базовых определений и теорем совокупность которых и представляет собой базовую основу для всех последующих видов искусственная нейронная сетьискусственных нейронных сетей Но как минимум необходимо понимание хотя бы с точки зрения Теория сигналовтеории сигналов являющееся оригинальным то есть описанное автором перцептрона Ф Розенблаттом

 Описание на основе сигналов 
ФайлThreshold functionsvgframerightПороговая функция реализуемая простыми S и Aэлементами
ФайлSignumsvgframerightПороговая функция реализуемая простым Rэлементом
Для начала определим составные элементы перцептрона которые являются частными случаями искусственный нейронискусственного нейрона с Искусственный нейронПороговая передаточная функцияпороговой передаточной функцией
 Простым Sэлементом сенсорным является чувствительный элемент который от воздействия какоголибо из видов энергии например света звука давления тепла и т п вырабатывает сигнал Если входной сигнал превышает некоторый порог θ на выходе элемента получаем 1 в противном случае  0ref nameelemdefРозенблатт Ф с 81ref
 Простым Aэлементом ассоциативным называется логический решающий элемент который даёт выходной сигнал 1 когда алгебраическая сумма его входных сигналов превышает некоторую пороговую величину θ говорят что элемент активный в противном случае выход равен нулюref nameelemdef 
 Простым Rэлементом реагирующим то есть действующим называется элемент который выдаёт сигнал 1 если сумма его входных сигналов является строго положительной и сигнал 1 если сумма его входных сигналов является строго отрицательной Если сумма входных сигналов равна нулю выход считается либо равным нулю либо неопределённымref nameelemdef 

Если на выходе любого элемента мы получаем 1 то говорят что элемент активен или возбуждён

Все рассмотренные элементы называются простыми так как они реализуют скачкообразные функции Розенблатт утверждал также что для решения более сложных задач могут потребоваться другие виды функций например Линейная функциялинейнаяref namenotsimpleРозенблатт Ф с 200ref

В результате Розенблатт ввёл следующие определения
 Перцептрон представляет собой сеть состоящую из S A Rэлементов с переменной матрицей взаимодействия W элементы которой mathw_ijmath  весовые коэффициенты определяемой последовательностью прошлых состояний активности сетиref namenotsimple ref namepercdefРозенблатт Ф с 82ref
 Перцептроном с последовательными связями называется система в которой все связи начинающиеся от элементов с логическим расстоянием d от ближайшего Sэлемента оканчиваются на элементах с логическим расстоянием d1 от ближайшего Sэлементаref namepercdef 
 Простым перцептроном называется любая система удовлетворяющая следующим пяти условиям
 в системе имеется только один Rэлемент естественно он связан всеми Aэлементами
 система представляет собой перцептрон с последовательными связями идущими только от Sэлементов к Aэлементам и от Aэлементов к Rэлементам
 веса всех связей от Sэлементов к Aэлементам SA связей неизменны
 время передачи каждой связи равно либо нулю либо фиксированной постоянной mathtaumath
 все активирующие функции S A Rэлементов имеют вид mathU_it  fa_itmath где matha_itmath  алгебраическая сумма всех сигналов поступающих одновременно на вход элемента mathu_imathref namenotsimple refРозенблатт Ф с 83ref
 Элементарным перцептроном называется простой перцептрон у которого все элементы  простые В этом случае его активизирующая функция имеет вид mathc_ijt  U_it  tauw_ijtmathref nameРозенблатт Ф с 93Розенблатт Ф с 93ref

Дополнительно можно указать на следующие концепции предложенные в книге и позднее развитые в рамках теории нейронных сетей
 Перцептрон с перекрёстными связями  это система в которой существуют связи между элементами одного типа S A или R находящиеся на одинаковом логическом расстоянии от Sэлементов причём все остальные связи  последовательного типаref namepercdef 
 Перцептрон с обратной связью  это система в которой существует хотя бы одна связь от логически более удалённого элемента к менее удалённомуref namepercdef  Согласно современной терминологии такие сети называются Рекуррентная нейронная сетьрекуррентными
 Перцептрон с переменными SA связями  это система в которой снято ограничение на фиксированные связи от Sэлементов к Aэлементам Доказано что путём оптимизации SA связей можно добиться значительного улучшения характеристик перцептронаref namesavarРозенблатт Ф с 230ref

 Описание на основе предикатов 
MainПерцептрон предикатное описание
Марвин Минский изучал свойства Параллельные вычисленияпараллельных вычислений частным случаем которых на то время был перцептрон Для анализа его свойств ему пришлось переизложить теорию перцептронов на язык предикатов Суть подхода заключалась в следующемref groupnbИзложение в этом разделе несколько упрощено изза сложности анализа на основе предикатовrefrefМинский Пейперт с 1118ref
 множеству сигналов от Sэлементов была сопоставлена переменная X
 каждому Aэлементу был сопоставлен предикат φX фи от икс названный частным предикатом
 каждому Rэлементу был сопоставлен предикат ψ пси зависящий от частных предикатов
 наконец перцептроном было названо устройство способное вычислять все предикаты типа ψ

Применительно к зрительному перцептрону переменная X символизировала образ какойлибо геометрической фигуры стимул Частный предикат позволял распознавать каждый свою фигуру Предикат ψ означал ситуацию когда линейная комбинация matha_1phi_1  ldots  a_nphi_nmath matha_imath  коэффициенты передачи превышала некоторый порог θ

Учёные выделили 5 семейств перцептронов обладающих по их мнению интересными свойствамиrefМинский Пейперт с 18ref
 Перцептроны ограниченные по диаметру  каждая фигура X распознаваемая частными предикатами не превосходит по диаметру некоторую фиксированную величину
 Перцептроны ограниченного порядка  каждый частный предикат зависит от ограниченного количества точек из X
 Перцептроны Гамбы  каждый частный предикат должен быть линейной пороговой функцией то есть миниперцептроном
 Случайные перцептроны  перцептроны ограниченного порядка где частные предикаты представляют собой случайно выбранные булевы функции В книге отмечается что именно эта модель наиболее подробно изучалась группой Розенблатта
 Ограниченные перцептроны  множество частных предикатов бесконечно а множество возможных значений коэффициентов matha_imath конечно

Хотя такой математический аппарат позволил применить анализ только к элементарному перцептрону Розенблатта он вскрыл много принципиальных ограничений для параллельных вычислений от которых не свободен ни один вид современных искусственных нейронных сетей

 Историческая классификация 
ФайлNeuroPNGthumb350pxАрхитектура многослойного перцептрона обоих подтипов
Понятие перцептрона имеет интересную но незавидную историю В результате неразвитой терминологии нейронных сетей прошлых лет резкой критики и непонимания задач исследования перцептронов а иногда и ложного освещения прессой изначальный смысл этого понятия исказился Сравнивая разработки Розенблатта и современные обзоры и статьи можно выделить 4 довольно обособленных класса перцептронов

 Перцептрон с одним скрытым слоем
 Это классический перцептрон которому посвящена большая часть книги Розенблатта и рассматриваемый в данной статье у него имеется по одному слою S A и Rэлементов

 Однослойный перцептрон
 Это модель в которой входные элементы напрямую соединены с выходными с помощью системы весов Является простейшей Искусственная нейронная сетьСети прямого распространения Feedforwardсетью прямого распространения  Линейный классификаторлинейным классификатором и частным случаем классического перцептрона в котором каждый Sэлемент однозначно соответствует одному Aэлементу SA связи имеют вес 1 и все Aэлементы имеют порог θ  1 Однослойные перцептроны фактически являются Искусственный нейронформальными нейронами то есть пороговыми элементами МакКаллока  Питтса Они имеют множество ограничений в частности они не могут идентифицировать ситуацию когда на их входы поданы разные сигналы задача XOR см Функциональные заблужденияниже

 Многослойный перцептрон РозенблаттаМногослойный перцептрон по Розенблатту
 Это перцептрон в котором присутствуют дополнительные слои Aэлементов Его анализ провёл Розенблатт в третьей части своей книги

 Многослойный перцептрон РумельхартаМногослойный перцептрон по Румельхарту
 Это перцептрон в котором присутствуют дополнительные слои Aэлементов причём обучение такой сети проводится по методу Метод обратного распространения ошибкиобратного распространения ошибки и обучаемыми являются все слои перцептрона в том числе SA Является частным случаем многослойного перцептрона Розенблатта

В настоящее время в литературе под термином перцептрон понимается чаще всего однослойный перцептрон langenSinglelayer perceptron причём существует распространённое заблуждение что именно этот простейший тип моделей предложил Розенблатт В противоположность однослойному ставят многослойный перцептрон langenMultilayer perceptron опять же чаще всего подразумевая многослойный перцептрон Румельхарта а не Розенблатта Классический перцептрон в такой дихотомии относят к многослойным

 Алгоритмы обучения 
Важным свойством любой нейронной сети является Машинное обучениеспособность к обучению Процесс обучения является процедурой настройки весов и порогов с целью уменьшения разности между желаемыми целевыми и получаемыми векторами на выходе В своей книге Розенблатт пытался классифицировать различные алгоритмы обучения перцептрона называя их системами подкрепления
 Система подкрепления  это любой набор правил на основании которых можно изменять с течением времени матрицу взаимодействия или состояние памяти перцептронаref nameroslearnРозенблатт Ф с 8588ref
Описывая эти системы подкрепления и уточняя возможные их виды Розенблатт основывался на идеях Хебб ДональдД Хебба об обучении предложенных им в 1949 годуref namehebb  которые можно перефразировать в следующее правило состоящее из двух частей
 Если два нейрона по обе стороны синапса соединения активизируются одновременно то есть синхронно то прочность этого соединения возрастает
 Если два нейрона по обе стороны синапса активизируются асинхронно то такой синапс ослабляется или вообще отмираетrefХайкин С 2006 с 96ref

 Обучение с учителем 
MainМетод коррекции ошибки
Классический метод обучения перцептрона  это метод коррекции ошибкиref nameteorshod  Он представляет собой такой вид Обучение с учителемобучения с учителем при котором вес связи не изменяется до тех пор пока текущая реакция перцептрона остаётся правильной При появлении неправильной реакции вес изменяется на единицу а знак  определяется противоположным от знака ошибки

Допустим мы хотим обучить перцептрон разделять два класса объектов так чтобы при предъявлении объектов первого класса выход перцептрона был положителен 1 а при предъявлении объектов второго класса  отрицательным 1 Для этого выполним следующий алгоритмref namelearning 
 Случайным образом выбираем пороги для Aэлементов и устанавливаем связи SA далее они изменяться не будут
 Начальные коэффициенты mathw_imath полагаем равными нулю
 Предъявляем обучающую выборку объекты например круги либо квадраты с указанием класса к которым они принадлежат
 Показываем перцептрону объект первого класса При этом некоторые Aэлементы возбудятся Коэффициенты mathw_imath соответствующие этим возбуждённым элементам увеличиваем на 1
 Предъявляем объект второго класса и коэффициенты mathw_imath тех Aэлементов которые возбудятся при этом показе уменьшаем на 1
 Обе части шага 3 выполним для всей обучающей выборки В результате обучения сформируются значения весов связей mathw_imath

Теорема сходимости перцептронаref nameteorshod  описанная и доказанная Ф Розенблаттом с участием Блока Джозефа Кестена и других исследователей работавших вместе с ним показывает что элементарный перцептрон обучаемый по такому алгоритму независимо от начального состояния весовых коэффициентов и последовательности появления стимулов всегда приведёт к достижению решения за конечный промежуток времени

 Обучение без учителя 
Кроме классического метода обучения перцептрона Розенблатт также ввёл понятие об Обучение без учителяобучении без учителя предложив следующий способ обучения
 Альфасистема подкрепления  это система подкрепления при которой веса всех активных связей mathc_ijmath которые ведут к элементу mathu_jmath изменяются на одинаковую величину r а веса неактивных связей за это время не изменяютсяrefРозенблатт Ф с 86ref
Затем с разработкой понятия многослойный перцептрон Румельхартамногослойного перцептрона альфасистема была модифицирована и её стали называть дельтаправило Модификация была проведена с целью сделать функцию обучения Производная функциидифференцируемой например сигмоидной что в свою очередь нужно для применения метода Градиентный спускградиентного спуска благодаря которому возможно обучение более одного слоя

 Метод обратного распространения ошибки 
MainАлгоритм обратного распространения ошибки
Для обучения многослойных сетей рядом учёных в том числе Дэвид РумельхартД Румельхартом был предложен Градиентный спускградиентный алгоритм обучения с учителем проводящий сигнал ошибки вычисленный выходами перцептрона к его входам слой за слоем Сейчас это самый популярный метод обучения многослойных перцептронов Его преимущество в том что он может обучить все слои нейронной сети и его легко просчитать локально Однако этот метод является очень долгим к тому же для его применения нужно чтобы передаточная функция нейронов была дифференцируемой При этом в перцептронах пришлось отказаться от бинарного сигнала и пользоваться на входе Вещественное числонепрерывными значениямиrefХайкин С 2006 с 225243 304316ref

 Традиционные заблуждения 
В результате популяризации искусственных нейронных сетей журналистами и маркетологами был допущен ряд неточностей которые при недостаточном изучении оригинальных работ по этой тематике неверно истолковывались молодыми на то время учёными В результате по сей день можно встретиться с недостаточно глубокой трактовкой функциональных возможностей перцептрона по сравнению с другими нейронными сетями разработанными в последующие годыкогда

 Терминологические неточности 
Самая распространённая ошибка связанная с терминологией это определение перцептрона как нейронной сети без скрытых слоёв однослойного перцептрона см Историческая классификациявыше Эта ошибка связана с недостаточно проработанной терминологией в области нейросетей на раннем этапе их разработки Ф Уоссерменом была сделана попытка определённым образом классифицировать различные виды нейронных сетей
начало цитаты
Как видно из публикаций нет общепринятого способа подсчёта числа слоёв в сети Многослойная сеть состоит из чередующихся множеств нейронов и весов Входной слой не выполняет суммирования Эти нейроны служат лишь в качестве разветвлений для первого множества весов и не влияют на вычислительные возможности сети По этой причине первый слой не принимается во внимание при подсчёте слоев и сеть считается двухслойной так как только два слоя выполняют вычисления Далее веса слоя считаются связанными со следующими за ними нейронами Следовательно слой состоит из множества весов со следующими за ними нейронами суммирующими взвешенные сигналыrefУоссермен Ф Нейрокомпьютерная техника Теория и практика 1992ref
конец цитаты
В результате такого представления перцептрон попал под определение однослойная нейронная сеть Отчасти это верно потому что у него нет скрытых слоёв обучающихся нейронов веса которых адаптируются к задаче И поэтому всю совокупность фиксированных связей системы из S к Aэлементам можно логически заменить набором модифицированных по жёсткому правилу новых входных сигналов поступающих сразу на Аэлементы устранив тем самым вообще первый слой связей Но тут как раз не учитывают что такая модификация превращает нелинейное представление задачи в линейное

Поэтому просто игнорирование не обучаемых слоёв с фиксированными связями в элементарном перцептроне это SA связи позволяет делать неправильные выводы о возможностях нейросети Так Минский поступил очень корректно переформулировав Аэлемент как предикат то есть функцию наоборот Уоссермен уже потерял такое представление и у него Аэлемент  просто вход почти эквивалентный Sэлементу При такой терминологической путанице упускается из виду тот факт что в перцептроне происходит отображение рецептивное полерецептивного поля Sэлементов на ассоциативное поле Аэлементов в результате чего и происходит преобразование любой линейно неразделимой задачи в линейно разделимую

 Функциональные заблуждения 
ФайлPerceptron XOR task v2svg300pxthumbРешение элементарным перцептроном задачи XOR Порог всех элементов θ  0
Большинство функциональных заблуждений сводятся к якобы невозможности решения перцептроном нелинейно разделяемой задачи Но вариаций на эту тему достаточно много рассмотрим главные из них

 Задача XOR 
Перцептрон не способен решить исключающее илизадачу XOR
 Очень распространённое и самое несерьёзное заявление На изображении справа показано решение этой задачи перцептроном Данное заблуждение возникает вопервых изза того что неправильно интерпретируют определение перцептрона данного Минским см Модель на основе предикатоввыше а именно предикаты сразу приравнивают входам хотя предикат у Минского  это функция идентифицирующая целый набор входных значенийref groupnbПредикат эквивалентен входу лишь в частном случае  только когда он зависит от одного аргументаref Вовторых изза того что классический перцептрон Розенблатта путают с однослойным перцептроном изза терминологической неточности описанной выше
Следует обратить особое внимание на то что однослойный перцептрон в современной терминологии и однослойный перцептрон в терминологии Уоссермана  разные объекты И объект изображённый на иллюстрации в терминологии Уоссермана есть двухслойный перцептрон

 Обучаемость линейно неразделимым задачам 
Выбором случайных весов можно достигнуть обучения и линейно неразделимым вообще любым задачам но только если повезёт и в новых переменных выходах Aнейронов задача окажется линейно разделимой Но может и не повезти

 Теорема сходимости перцептронаref nameteorshod  доказывает что нет и не может быть никакого может и не повезти при равенстве Аэлементов числу стимулов и не особенной Gматрица перцептронаGматрице  вероятность решения равна 100 То есть при отображениеотображении рецепторного поля на ассоциативное поле большей на одну Размерность пространстваразмерности случайным нелинейным Оператор математикаоператором нелинейная задача превращается в линейно разделимую А следующий обучаемый слой уже находит линейное решение в другом пространстве входов

 Например обучение перцептрона для решения задачи XOR см на иллюстрации проводится следующими этапами
div aligncenter
 classwikitable
rowspan2Веса
colspan9Итерации

colspan31
2
colspan23
colspan24
5

w1
 width40  0
 width40  1
 width40  1
 width40  1
 width40  1
 width40  2
 width40  2
 width40  2
 width40  2

w2
  0 0 1 1 1  1  1 2  2

w3
 1 0 1 0 1 0 1 0 1

Входные сигналы x y
  1 1 0 1 1 0 1 1  1 1 0 1  1 1 1 0  1 1
div

 Обучаемость на малом числе примеров 
Если в задаче размерность входов довольно высока а обучающих примеров мало то в таком слабо заполненном пространстве число удач может и не оказаться малым Это свидетельствует лишь о частном случае пригодности перцептрона а не его универсальности

 Данный аргумент легко проверить на тестовой задаче под названием шахматная доска или губка с водойrefБонгард М М с 29refref groupnbБонгард Михаил МоисеевичМ М Бонгард считает эту задачу наисложнейшей для проведения гиперплоскости в пространстве рецепторовref
ЗадачаДана цепочка из 2N единиц или нулей параллельно поступающих на входы перцептрона Если эта цепочка является зеркально симметричной относительно центра то на выходе 1 иначе 0 Обучающие примеры  все это важно math22Nmath цепочек
 Могут быть вариации данной задачи например
ЗадачаВозьмём битовое изображениечёрнобелое изображение размером 256256 элементов пикселов Входными данными для перцептрона будут координаты точки 8 бит  8 бит итого нужно 16 Sэлементов на выходе потребуем цвет точки Обучаем перцептрон всем точкам всему изображению В итоге имеем 65nbsp536 различных пар стимулреакция Обучить без ошибок
 Если данный аргумент справедлив то перцептрон не сможет ни при каких условиях обучиться не делая ни одной ошибки Иначе перцептрон не ошибётся ни разу

 На практике оказывается что данная задача очень проста для перцептрона чтобы её решить перцептрону достаточно 1500 Аэлементов вместо полных 65 536 необходимых для любой задачи При этом число Итерацияитераций порядка 1000 При 1000 Аэлементов перцептрон не сходится за 10 000 итераций Если же увеличить число Аэлементов до 40 000 то схождения можно ожидать за 3080 итераций

 Такой аргумент появляется изза того что данную задачу путают с задачей Минского о предикате чётностьref namechetnostМинский М Пейперт С с 59ref

 Стабилизация весов и сходимость 
В перцептроне Розенблатта столько Аэлементов сколько входов И сходимость по Розенблатту это стабилизация весов

 У Розенблатта читаем
начало цитаты
Если число стимулов в пространстве W равно n  N то есть больше числа Аэлементов элементарного перцептрона то существует некоторая классификация nobrСW для которой решения не существуетrefРозенблатт Ф с 101ref
конец цитаты
 Отсюда следует что
 у Розенблатта число Аэлементов равно числу стимулов обучающих примеров а не числу входов
 сходимость по Розенблатту это не стабилизация весов а наличие всех требуемых классификаций то есть по сути отсутствие ошибок

 Экспоненциальный рост числа скрытых элементов 
Если весовые коэффициенты к элементам скрытого слоя Аэлементам фиксированы то необходимо чтобы количество элементов скрытого слоя либо их сложность экспоненциально возрастало с ростом размерности задачи числа рецепторов Тем самым теряется их основное преимущество  способность решать задачи произвольной сложности при помощи простых элементов

 Розенблаттом было показано что число Аэлементов зависит только от числа стимулов которые нужно распознать см предыдущий пункт или теорема сходимости перцептронатеорему сходимости перцептрона Таким образом при возрастании числа рецепторов если количество Аэлементов фиксировано непосредственно не зависит возможность перцептрона к решению задач произвольной сложности

 Такое заблуждение происходит от следующей фразы Минского
начало цитаты
При исследовании предиката чётность мы видели что коэффициенты могут расти с ростом R числа точек на изображении экспоненциальноrefМинский Пейперт с 155 189 недословно упрощённо для выразительностиref
конец цитаты
 Кроме того Минский исследовал и другие предикаты например равенство Но все эти предикаты представляют собой достаточно специфическую задачу на обобщение а не на распознавание или прогнозирование Так например чтобы перцептрон мог выполнять предикат четность  он должен сказать чётно или нет число чёрных точек на чёрнобелом изображении а для выполнения предиката равенство  сказать равна ли правая часть изображения левой Ясно что такие задачи выходят за рамки задач распознавания и прогнозирования и представляют собой задачи на обобщение или просто на подсчёт определённых характеристик Это и было убедительно показано Минским и является ограничением не только перцептронов но и всех Параллельные вычисленияпараллельных алгоритмов которые не способны быстрее последовательных алгоритмов вычислить такие предикаты

 Поэтому такие задачи ограничивают возможности всех нейронных сетей и перцептронов в частности но это никак не связанно с фиксированными связями первого слоя так как вопервых речь шла о величине коэффициентов связей второго слоя а вовторых вопрос только в эффективности а не принципиальной возможности То есть перцептрон можно обучить и этой задаче но требуемые для этого ёмкость памяти и скорость обучения будут больше чем при применении простого последовательного алгоритма Введение же обучаемых весовых коэффициентов в первом слое лишь ухудшит положение дел ибо потребует большего времени обучения потому что переменные связи между S и A скорее препятствуют чем способствуют процессу обученияrefРозенблатт стр 239ref Причём при подготовке перцептрона к задаче распознавания стимулов особого типа для сохранения эффективности потребуются особые условия стохастического обученияref nameautogenerated1Розенблатт стр 242ref что было показано Розенблаттом в экспериментах с Перцептрон с переменными SA связямиперцептроном с переменными SA связями

 Возможности и ограничения модели 
MainВозможности и ограничения перцептронов

 Возможности модели 
ФайлEdge approximationsvgthumb200pxleftПример Классификацияклассификации объектов Зелёная линия  граница классов
Сам Розенблатт рассматривал перцептрон прежде всего как следующий важный шаг в сторону исследования и использования нейронных сетей а не как оконченный вариант Может ли машина мыслитьмашины способной мыслитьref groupnbНа первых этапах развития науки об искусственном интеллекте её задача рассматривалась в абстрактном смысле  создание систем напоминающих по разуму человека см искусственный общий интеллект Современные формулировки задач в ИИ как правило более аккуратныref Ещё в предисловии к своей книге он отвечая на критику отмечал что программа по исследованию перцептрона связана главным образом не с изобретением устройств обладающих искусственным интеллектом а с изучением физических структур и нейродинамических принциповrefРозенблатт Ф с 18ref

Розенблатт предложил ряд Психологический тестпсихологических тестов для определения возможностей нейросетей эксперименты по Кластеризацияразличению Обобщение понятийобобщению по Поиск закономерностейраспознаванию последовательностей Абстрагированиеобразованию абстрактных понятий формированию и свойствам Самосознаниесамосознания Творчествотворческого Воображениевоображения и другиеrefРозенблатт Ф с 7077ref Некоторые из этих экспериментов далеки от современных возможностей перцептронов поэтому их развитие происходит больше Философияфилософски в пределах направления коннективизма Тем не менее для перцептронов установлены два важных факта находящие применение в практических задачах возможность классификацияклассификации объектов и возможность аппроксимацияаппроксимации границ классов и функцийref nameezhovсм Ежов А А Шумский С А Нейрокомпьютинг 2006  Лекция 3 Обучение с учителем Распознавание образовref

Важным свойством перцептронов является их способность к обучению причём по довольно простому и эффективному алгоритму см Алгоритмы обучениявыше

 Ограничения модели 

mainПерцептроны книга

ФайлInvariant image recognitionsvgthumb180pxНекоторые задачи которые перцептрон не способен решить 1 2  преобразования группы переносов 3  из какого количества частей состоит фигура 4  внутри какого объекта нет другой фигуры 5  какая фигура внутри объектов повторяется два раза 3 4 5  задачи на определение связности фигур

Сам Розенблатт выделил два Фундаментальные ограниченияфундаментальных ограничения для трёхслойных перцептронов состоящих из одного Sслоя одного Aслоя и Rслоя отсутствие у них способности к обобщению своих характеристик на новые стимулы или новые ситуации а также неспособность анализировать сложные ситуации во внешней среде путём расчленения их на более простыеref nameРозенблатт Ф с 93

В 1969 году Минский Марвин ЛиМарвин Минский и Паперт СеймурСеймур Паперт опубликовали книгу Перцептроны где математически показали что перцептроны подобные розенблаттовским принципиально не в состоянии выполнять многие из тех функций которые хотели получить от перцептронов К тому же в то время была слабо развита теория о параллельных вычислениях а перцептрон полностью соответствовал принципам таких вычислений По большому счёту Минский показал преимущество последовательных вычислений перед Параллельные вычисленияпараллельным в определённых классах задач связанных с инвариантным представлением Его критику можно разделить на три темы

 Перцептроны имеют ограничения в задачах связанных с инвариантным представлением образов то есть независимым от их положения на сенсорном поле и относительно других фигур Такие задачи возникают например если нам требуется построить машину для чтения Символпечатных букв или цифр так чтобы эта машина могла распознавать их независимо от положения на странице то есть чтобы на решение машины не оказывали влияния Параллельный переносперенос поворот растяжениесжатие символовref nameinvariant  или если нам нужно определить из скольких частей состоит фигураrefМинский М Пейперт С с 7698ref или находятся ли две фигуры рядом или нетrefМинский М Пейперт С с 113116ref Минским было доказано что этот тип задач невозможно полноценно решить с помощью параллельных вычислений в том числе  перцептрона
 Перцептроны не имеют функционального преимущества над Аналитические методыаналитическими методами например Статистические методыстатистическими в задачах связанных с прогнозированиемrefМинский М Пейперт С с 192214ref Тем не менее в некоторых случаях они представляют более простой и Производительностьпроизводительный метод Интеллектуальный анализ данныханализа данных
 Было показано что некоторые задачи в принципе могут быть решены перцептроном но могут потребовать нереально Теория сложности вычисленийбольшого времениrefМинский Пейперт с 163187ref или нереально большой Оперативная памятьпамятиrefМинский Пейперт с 153162ref

Книга Минского и Паперта существенно повлияла на пути развития науки об искусственном интеллекте так как переместила научный интерес и субсидии правительственных организаций США на другое направление исследований  Искусственный интеллектсимвольный подход в ИИ

 Применение перцептронов 
Здесь будут показаны только основы практического применения перцептрона на двух различных задачах Задача Прогнозированиепрогнозирования и эквивалентная ей задача Распознавание образовраспознавания образов требует высокой точности а задача интеллектуальный агентуправления агентами  высокой скорости обучения Поэтому рассматривая эти задачи можно полноценно ознакомиться с возможностями перцептрона однако этим далеко не исчерпываются варианты его использования

В практических задачах от перцептрона потребуется возможность выбора более чем из двух вариантов а значит на выходе у него должно находиться более одного Rэлемента Как показано Розенблаттом характеристики таких систем не отличаются существенно от характеристик элементарного перцептронаrefРозенблатт Ф с 219224ref

 Прогнозирование и распознавание образов 
ФайлLearningagentrusvgthumb350pxrightВзаимодействие Интеллектуальный агентобучающегося агента со средой Важной частью такой системы являются обратные связи
В этих задачах от перцептрона требуется установить принадлежность объекта к какомулибо классу по его параметрам например по внешнему виду форме силуэту Причём точность распознавания будет во многом зависеть от представления выходных реакций перцептрона Здесь возможны три типа кодирования Конфигурационное кодированиеконфигурационное Позиционное кодированиепозиционное и гибридное Позиционное кодирование когда каждому классу соответствует свой Rэлемент даёт более точные результаты чем другие виды Такой тип использован например в работе Э Куссуль и др Перцептроны Розенблатта для распознавания рукописных цифр Однако оно неприменимо в тех случаях когда число классов значительно например несколько сотен В таких случаях можно применять гибридное конфигурационнопозиционное кодирование как это было сделано в работе С Яковлева Система распознавания движущихся объектов на базе искусственных нейронных сетей

 Управление агентами 
В искусственном интеллекте часто рассматриваются обучающиеся Адаптивная системаадаптирующиеся к окружающая средаокружающей среде агенты При этом в условия неопределённостиусловиях неопределённости становится важным анализировать не только текущую информацию но и общий контекст ситуации в которую попал агент поэтому здесь применяются перцептроны с обратной связьюrefЯковлев С С  Использование принципа рекуррентности Джордана в перцептроне Розенблатта Журнал АВТОМАТИКА И ВЫЧИСЛИТЕЛЬНАЯ ТЕХНИКА Рига 2009 Virtual Laboratory Wikiref Кроме того в некоторых задачах становится важным повышение скорости обучения перцептрона например с помощью моделирования Рефрактерный периодрефрактерностиrefЯковлев С С Investigation of Refractoriness principle in Recurrent Neural Networks Scientific proceedings of Riga Technical University Issue 5 Vol36 RTU Riga 2008 P 4148  Исследование принципа рефрактерности в рекуррентных нейронных сетях перевод ref

После периода известного как Зима искусственного интеллекта интерес к кибернетическим моделям возродился в 1980е1980х годах так как сторонники символьного подхода в ИИ так и не смогли подобраться к решению вопросов о Понимании и Значении изза чего машинный перевод и техническое распознавание образов до сих пор обладает неустранимыми недостатками Сам Минский публично выразил сожаление что его выступление нанесло урон концепции перцептронов хотя книга лишь показывала недостатки отдельно взятого устройства и некоторых его вариаций Но в основном ИИ стал синонимом символьного подхода который выражался в составлении все более сложных программ для компьютеров моделирующих сложную деятельность человеческого мозга

 См также 
 Свёрточная нейронная сеть
 Биокомпьютинг
 Байесовская сеть доверия
 Когнитрон
 История искусственного интеллекта
 Паттерн психология

 Примечания 
примечанияgroupnb

 Источники 
примечания2

 Литература 
 книга
автор           Бонгард Михаил МоисеевичБонгард М М
заглавие        Проблема узнавания
оригинал        
ссылка          
место           М
издательство    Наука
год             1967
страниц         320

 книга
автор           Брюхомицкий Ю А
заглавие        Нейросетевые модели для систем информационной безопасности Учебное пособие
ссылка          
место           Таганрог
издательство    Издво ТРТУ
год             2005
страниц         160

 статья
 автор          Маккалок УорренМакКаллок У С Питтс УорренПиттс В
 заглавие       Логическое исчисление идей относящихся к нервной активности
 оригинал       A logical calculus of the ideas immanent in nervous activity
 ссылка         
 издание        Автоматы
 тип            Сб
 место          М
 год            1956
 страницы       363384

 книга
автор           Минский Марвин ЛиМинский М Паперт СеймурПейперт С
заглавие        Персептроны
оригинал        Perceptrons
ссылка          
место           М
издательство    Мир
год             1971
страниц         261

 книга
автор           Розенблатт ФрэнкРозенблатт Ф
заглавие        Принципы нейродинамики Перцептроны и теория механизмов мозга
оригинал        Principles of Neurodynamic Perceptrons and the Theory of Brain Mechanisms
ссылка          
место           М
издательство    Мир
год             1965
страниц         480

 книга
автор           Уоссермен Ф
заглавие        Нейрокомпьютерная техника Теория и практика
оригинал        Neural Computing Theory and Practice
ссылка          
место           М
издательство    Мир
год             1992
страниц         240
isbn            5030021159

 книга
автор           Хайкин С
заглавие        Нейронные сети Полный курс
оригинал        Neural Networks A Comprehensive Foundation
ссылка          
издание         2е изд
место           М
издательство    Вильямс издательствоВильямс
год             2006
страниц         1104
isbn            0132733501

 статья
 автор          Яковлев С С
 заглавие       Система распознавания движущихся объектов на базе искусственных нейронных сетей
 ссылка         
 издание        ИТК НАНБ
 место          Минск
 год            2004
 страницы       230234

 статья
 автор          Kussul E Baidyk T Kasatkina L Lukovich V
 оригинал       Rosenblatt Perceptrons for Handwritten Digit Recognition
 заглавие       Перцептроны Розенблатта для распознавания рукописных цифр
 ссылка         
 издание        IEEE
 год            2001
 страницы       15161520
 ISBN           0780370449
 refen
 статья
 автор          Stormo G D Schneider T D Gold L Ehrenfeucht A
 оригинал       Use of the Perceptron algorithm to distinguish transational initiation sites in E coli
 заглавие       Использование перцептрона для выделения сайтов инициации в E coli
 ссылка         
 издание        Nucleic Acids Research
 год            1982
 страницы       P 29973011
 ISBN           
 refen

 Ссылки 