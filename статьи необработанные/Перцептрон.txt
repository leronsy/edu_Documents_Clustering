{'batchcomplete': '', 'query': {'pages': {'633380': {'pageid': 633380, 'ns': 0, 'title': 'Перцептрон', 'revisions': [{'contentformat': 'text/x-wiki', 'contentmodel': 'wikitext', '*': '[[Файл:Perceptron-ru.svg|thumb|300px|Логическая схема перцептрона с тремя выходами]]\n\n\'\'\'Перцептро́н\'\'\', или \'\'\'пер\'\'с\'\'ептрон\'\'\'<ref group="nb">Вариант «перцептрон»\xa0— изначальный, используется в переводе книги Розенблатта (1965), также в справочнике: {{книга|автор = |часть = |заглавие = Толковый словарь по искусственному интеллекту|оригинал = |ссылка = http://raai.org/library/tolk/aivoc.html#L400|ответственный = Авторы-составители А.&nbsp;Н.&nbsp;Аверкин, М.&nbsp;Г.&nbsp;Гаазе-Рапопорт, [[Поспелов, Д. А.|Д.&nbsp;А.&nbsp;Поспелов]]|издание = |место = М.|издательство = Радио и связь|год = 1992|том = |страницы = |страниц = 256|серия = |isbn = }} Вариант «персептрон» встречается чаще, он возник при переводе книги Минского и Пейперта (1971); см. также: {{книга|автор = |часть = |заглавие = Энциклопедия кибернетики. Том 2. Мих—Яч|оригинал = |ссылка = http://www.krelib.com/enciklopedii/3626|ответственный = |издание = |место = Киев|издательство = Гл. изд. УСЭ|год = 1974|том = |страницы = 156—158|страниц = |серия = |isbn = }}</ref> ({{lang-en|perceptron}} от {{lang-la|perceptio}}\xa0— [[восприятие]]; {{lang-de|Perzeptron}})\xa0— [[Имитационное моделирование|математическая]] или [[компьютерное моделирование|компьютерная модель]] [[Восприятие|восприятия]] [[Информация|информации]] [[мозг]]ом ([[кибернетика|кибернетическая]] \'\'[[модель мозга]]\'\'), предложенная [[Розенблатт, Фрэнк|Фрэнком Розенблаттом]] в [[1957 год]]у и впервые реализованная в виде электронной машины [[Марк-1|«Марк-1»]]<ref group="nb">«Марк-1», в частности, был системой, имитирующей [[Зрительная система|человеческий глаз]] и его взаимодействие с мозгом.</ref> в [[1960 год]]у. Перцептрон стал одной из первых моделей \'\'[[Искусственная нейронная сеть|нейросетей]]\'\', а «Марк-1»\xa0— первым в мире \'\'[[нейрокомпьютер]]ом\'\'.\n\nПерцептрон состоит из трёх типов элементов, а именно: поступающие от \'\'\'[[датчик]]ов\'\'\' [[сигнал]]ы передаются \'\'\'[[ассоциативная память|ассоциативным]]\'\'\' элементам, а затем \'\'\'реагирующим\'\'\' элементам. Таким образом, перцептроны позволяют создать набор [[Ассоциация (психология)|«ассоциаций»]]<!-- по-моему, тут уж на «рефлекс» надо ссылаться, проще --Incnis Mrsi --> между входными [[стимул]]ами и необходимой [[Реакция (биология)|реакцией]] на выходе. В биологическом плане это соответствует преобразованию, например, зрительной информации в [[Физиологическая реакция|физиологический ответ]] от двигательных [[Нейроны|нейронов]]. Согласно современной терминологии, перцептроны могут быть классифицированы как искусственные нейронные сети:\n# с одним скрытым слоем;<ref group="nb">«Трёхслойные» по классификации, принятой у Розенблатта, и «двухслойные» по современной системе обозначений\xa0— с той особенностью, что первый слой не обучаемый.</ref>\n# с [[Искусственный нейрон#Пороговая передаточная функция|пороговой передаточной функцией]];\n# с [[Искусственная нейронная сеть#Сети прямого распространения (Feedforward)|прямым распространением сигнала]].\n\nНа фоне роста популярности нейронных сетей в [[1969 год]]у вышла книга [[Минский, Марвин|Марвина Минского]] и [[Паперт, Сеймур|Сеймура Паперта]], которая показала принципиальные ограничения перцептронов. Это привело к смещению интереса исследователей [[Искусственный интеллект|искусственного интеллекта]] в противоположную от нейросетей область [[символьные вычисления|символьных вычислений]]<ref group="nb">К символьному подходу, относятся, например, создание [[Экспертная система|экспертных систем]], организация [[База знаний|баз знаний]], [[Обработка естественного языка|анализ текстов]].</ref>. Кроме того, из-за сложности [[Математика|математического]] исследования перцептронов, а также отсутствия общепринятой терминологии, возникли различные [[#Традиционные заблуждения|неточности и заблуждения]].\n\nВпоследствии интерес к нейросетям, и в частности, работам Розенблатта, возобновился. Так, например, сейчас стремительно развивается [[биокомпьютинг]], который в своей теоретической основе вычислений, в том числе, базируется на нейронных сетях, а перцептрон воспроизводят на основе [[Бактериородопсин-содержащие пленки|бактериородопсин-содержащих пленок]].\n\n== Появление перцептрона ==\n\n[[Файл:Single layer perceptron.png|thumb|left|180px|Схема [[Искусственный нейрон|искусственного нейрона]]\xa0— базового элемента любой нейронной сети]]\n\nВ [[1943 год]]у в своей статье «Логическое исчисление идей, относящихся к нервной активности»<ref>{{статья | автор = {{lang|en| Warren S. McCulloch and Walter Pitts}}. | заглавие = Логическое исчисление идей, относящихся к нервной активности\n| оригинал = [http://www.springerlink.com/content/61446605110620kg/fulltext.pdf {{lang|en|A logical calculus of the ideas immanent in nervous activity}}] | ссылка = http://www.raai.org/library/books/mcculloch/mcculloch.pdf | издание = {{lang|en|Bulletin of Mathematical Biology}} | место = {{lang|en|New York}} | издательство = {{lang|en|Springer New York}} | год = 1943 | выпуск = | том = 5 | номер = 4 | страницы = 115—133 | isbn = }}</ref> [[Маккалок, Уоррен|Уоррен Мак-Каллок]] и [[Питтс, Уолтер|Уолтер Питтс]] предложили понятие [[Искусственная нейронная сеть|искусственной нейронной сети]]. В частности, ими была предложена модель [[Искусственный нейрон|искусственного нейрона]]. [[Хебб, Дональд|Дональд Хебб]] в работе «Организация поведения»<ref name="hebb">{{книга | автор = {{lang|en|Donald Olding Hebb}}. | заглавие = {{lang|en|The Organization of Behavior: A Neuropsychological Theory}} | издательство = {{lang|en|Wiley}} | год = 1949 | allpages = 335 }} Современное издание: {{книга | автор = {{lang|en|Donald Olding Hebb}}. | заглавие = {{lang|en|The Organization of Behavior: A Neuropsychological Theory}} | ссылка = https://books.google.com/books?id=VNetYrB8EBoC | издательство = {{lang|en|Lawrence Erlbaum Associates}} | год = 2002 | allpages = 335 | isbn = 0805843000, ISBN 978-0-8058-4300-2 }}</ref> [[1949 год]]а описал основные принципы обучения нейронов.\n\nЭти идеи несколько лет спустя развил американский [[нейрофизиолог]] [[Розенблатт, Фрэнк|Фрэнк Розенблатт]]. Он предложил схему устройства, [[Имитационное моделирование|моделирующего]] процесс [[органы чувств|человеческого восприятия]], и назвал его «перцептроном». Перцептрон передавал сигналы от [[фотоэлемент]]ов, представляющих собой сенсорное поле, в блоки электромеханических ячеек памяти. Эти ячейки соединялись между собой случайным образом в соответствии с принципами [[коннективизм]]а. В [[1957 год]]у в Корнеллской Лаборатории Аэронавтики было успешно завершено моделирование работы перцептрона на компьютере [[IBM 704]], а два года спустя, [[23 июня]] [[1960 год]]а в [[Корнеллский университет|Корнеллском университете]], был продемонстрирован первый [[нейрокомпьютер]]\xa0— \'\'\'«Марк-1»\'\'\', который был способен распознавать некоторые буквы английского алфавита<ref>[http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html Perceptrons: An Associative Learning Network]</ref><ref>[http://www.e-ink.ru/article/iui/poavlenie_perceptrona.htm Появление перцептрона]</ref>.\n\n[[Файл:Rosenblatt.jpg|thumb|right|200px|Фрэнк Розенблатт со своим творением\xa0— «Марк-1».]]\nЧтобы «научить» перцептрон классифицировать образы, был разработан специальный итерационный метод обучения проб и ошибок, напоминающий процесс обучения человека\xa0— [[метод коррекции ошибки]]<ref name="learning">[http://www.iskint.ru/?xid=books/sotnik/-part3 Системы распознавания образов]</ref>. Кроме того, при распознании той или иной буквы перцептрон мог выделять характерные особенности буквы, статистически чаще встречающиеся, чем малозначимые отличия в индивидуальных случаях. Тем самым, перцептрон был способен [[Обобщение понятий|обобщать]] буквы, написанные различным образом (почерком), в один обобщённый образ. Однако возможности перцептрона были ограниченными: машина не могла надёжно распознавать частично закрытые буквы, а также буквы иного размера, расположенные со сдвигом или поворотом, нежели те, которые использовались на этапе её обучения<ref name="invariant">Минский М., Пейперт С., с. 50.</ref>.\n\nОтчёт по первым результатам появился ещё в [[1958 год]]у\xa0— тогда Розенблаттом была опубликована статья «Перцептрон: Вероятная модель хранения и организации информации в головном мозге»<ref>[http://www.manhattanrarebooks-science.com/rosenblatt.htm The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain]</ref>. Но подробнее свои теории и предположения относительно процессов восприятия и перцептронов он описывает [[1962 год]]у в книге «Принципы нейродинамики: Перцептроны и теория механизмов мозга». В книге он рассматривает не только уже готовые модели перцептрона с одним скрытым слоем, но и [[Многослойный перцептрон Розенблатта|многослойных перцептронов]] с [[Перцептроны с перекрестными связями|перекрёстными]] (третья глава) и [[Перцептроны с обратной связью|обратными]] (четвёртая глава) связями. В книге также вводится ряд важных идей и теорем, например, доказывается [[теорема сходимости перцептрона]]<ref name="teorshod">Розенблатт Ф., с. 102.</ref>.\n\n== Описание элементарного перцептрона ==\n[[Файл:Perceptron physical implementation-ru.svg|280px|thumb|left|Поступление сигналов с сенсорного поля в решающие блоки элементарного перцептрона в его физическом воплощении.]]\n[[Файл:Simple perceptron.svg|thumb|280px|left|Логическая схема элементарного перцептрона. Веса S—A связей могут иметь значения −1, +1 или 0 (то есть отсутствие связи). Веса A—R связей W могут быть любыми.]]\nЭлементарный перцептрон состоит из элементов трёх типов: S-элементов, A-элементов и \'\'одного\'\' R-элемента. S-элементы\xa0— это слой сенсоров или рецепторов. В физическом воплощении они соответствуют, например, светочувствительным клеткам [[Сетчатка|сетчатки глаза]] или [[фоторезистор]]ам матрицы камеры. Каждый рецептор может находиться в одном из двух состояний\xa0— \'\'покоя\'\' или \'\'возбуждения\'\', и только в последнем случае он передаёт единичный сигнал в следующий слой, ассоциативным элементам.\n\nA-элементы называются ассоциативными, потому что каждому такому элементу, как правило, соответствует целый набор (ассоциация) S-элементов. A-элемент активизируется, как только количество сигналов от S-элементов на его входе превысило некоторую величину θ<ref group="nb">Формально A-элементы, как и R-элементы, представляют собой [[сумматор]]ы с \'\'порогом\'\', то есть [[Искусственный нейрон|одиночные нейроны]].</ref>. Таким образом, если набор соответствующих S-элементов располагается на сенсорном поле в форме буквы «Д», A-элемент активизируется, если достаточное количество рецепторов сообщило о появлении «белого пятна света» в их окрестности, то есть A-элемент будет как бы ассоциирован с наличием/отсутствием буквы «Д» в некоторой области.\n\nСигналы от возбудившихся A-элементов, в свою очередь, передаются в сумматор R, причём сигнал от i-го ассоциативного элемента передаётся с коэффициентом <math>w_{i}</math><ref>\'\'Фомин, С. В., Беркинблит, М. Б.\'\' [http://www.library.biophys.msu.ru/FominBerk/main.htm Математические проблемы в биологии]</ref>. Этот коэффициент называется \'\'весом\'\' A—R связи.\n\nТак же как и A-элементы, R-элемент подсчитывает сумму значений входных сигналов, помноженных на веса ([[Линейная форма|линейную форму]]). R-элемент, а вместе с ним и элементарный перцептрон, выдаёт «1», если линейная форма превышает порог θ, иначе на выходе будет «−1». Математически, функцию, реализуемую R-элементом, можно записать так:\n: <math>f(x) = \\operatorname{sign}(\\sum_{i=1}^{n} w_i x_i - \\theta)</math>\n\n[[Машинное обучение|Обучение]] элементарного перцептрона состоит в изменении весовых коэффициентов <math>w_i</math> связей A—R. Веса связей S—A (которые могут принимать значения {−1; 0; +1}) и значения порогов A-элементов выбираются случайным образом в самом начале и затем не изменяются. (Описание алгоритма см. [[#Алгоритмы обучения|ниже]].)\n\nПосле обучения перцептрон готов работать в режиме \'\'[[распознавание образов|распознавания]]\'\'<ref>Розенблатт, Ф., с. 158—162.</ref> или \'\'[[обобщение понятий|обобщения]]\'\'<ref>Розенблатт, Ф., с. 162—163.</ref>. В этом режиме перцептрону предъявляются ранее неизвестные ему объекты, и перцептрон должен установить, к какому классу они принадлежат. Работа перцептрона состоит в следующем: при предъявлении объекта возбудившиеся A-элементы передают сигнал R-элементу, равный сумме соответствующих коэффициентов <math>w_i</math>. Если эта сумма положительна, то принимается решение, что данный объект принадлежит к первому классу, а если она отрицательна\xa0— то ко второму<ref>\'\'Брюхомицкий Ю. А.\'\' Нейросетевые модели для систем информационной безопасности, 2005.</ref>.\n\n== Основные понятия теории перцептронов ==\nСерьёзное ознакомление с теорией перцептронов требует знания базовых определений и теорем, совокупность которых и представляет собой базовую основу для всех последующих видов [[искусственная нейронная сеть|искусственных нейронных сетей]]. Но, как минимум, необходимо понимание хотя бы с точки зрения [[Теория сигналов|теории сигналов]], являющееся оригинальным, то есть описанное автором перцептрона Ф. Розенблаттом.\n\n=== Описание на основе сигналов ===\n[[Файл:Threshold function.svg|frame|right|Пороговая функция, реализуемая простыми S- и A-элементами.]]\n[[Файл:Signum.svg|frame|right|Пороговая функция, реализуемая простым R-элементом.]]\nДля начала определим составные элементы перцептрона, которые являются частными случаями [[искусственный нейрон|искусственного нейрона]] с [[Искусственный нейрон#Пороговая передаточная функция|пороговой передаточной функцией]].\n* Простым \'\'\'S-элементом\'\'\' (сенсорным) является чувствительный элемент, который от воздействия какого-либо из видов энергии (например, света, звука, давления, тепла и\xa0т.\xa0п.) вырабатывает сигнал. Если входной сигнал превышает некоторый порог θ, на выходе элемента получаем +1, в противном случае\xa0— 0<ref name="elemdef">Розенблатт Ф., с. 81.</ref>.\n* Простым \'\'\'A-элементом\'\'\' (ассоциативным) называется логический решающий элемент, который даёт выходной сигнал +1, когда [[алгебраическая сумма]] его входных сигналов превышает некоторую пороговую величину θ (говорят, что элемент \'\'активный\'\'), в противном случае выход равен нулю<ref name="elemdef" />.\n* Простым \'\'\'R-элементом\'\'\' (реагирующим, то есть действующим) называется элемент, который выдаёт сигнал +1, если сумма его входных сигналов является строго положительной, и сигнал −1, если сумма его входных сигналов является строго отрицательной. Если сумма входных сигналов равна нулю, выход считается либо равным нулю, либо неопределённым<ref name="elemdef" />.\n\nЕсли на выходе любого элемента мы получаем 1, то говорят, что элемент \'\'\'активен\'\'\' или \'\'\'возбуждён\'\'\'.\n\nВсе рассмотренные элементы называются \'\'простыми\'\', так как они реализуют \'\'скачкообразные функции\'\'. Розенблатт утверждал также, что для решения более сложных задач могут потребоваться другие виды функций, например, [[Линейная функция|линейная]]<ref name="notsimple">Розенблатт, Ф., с. 200.</ref>.\n\nВ результате Розенблатт ввёл следующие определения:\n* \'\'\'Перцептрон\'\'\' представляет собой сеть, состоящую из S-, A-, R-элементов, с переменной матрицей взаимодействия W (элементы которой <math>w_{ij}</math>\xa0— весовые коэффициенты), определяемой последовательностью прошлых состояний активности сети<ref name="notsimple" /><ref name="percdef">Розенблатт Ф., с. 82.</ref>.\n* \'\'\'Перцептроном с последовательными связями\'\'\' называется система, в которой все связи, начинающиеся от элементов с логическим расстоянием d от ближайшего S-элемента, оканчиваются на элементах с логическим расстоянием d+1 от ближайшего S-элемента<ref name="percdef" />.\n* \'\'\'Простым перцептроном\'\'\' называется любая система, удовлетворяющая следующим пяти условиям:\n*# в системе имеется только один R-элемент (естественно, он связан всеми A-элементами);\n*# система представляет собой перцептрон с последовательными связями, идущими только от S-элементов к A-элементам и от A-элементов к R-элементам;\n*# веса всех связей от S-элементов к A-элементам (S—A связей) неизменны;\n*# время передачи каждой связи равно либо нулю, либо фиксированной постоянной <math>\\tau</math>;\n*# все активирующие функции S-, A-, R-элементов имеют вид <math>U_{i}(t) = f(a_{i}(t))</math>, где <math>a_{i}(t)</math>\xa0— алгебраическая сумма всех сигналов, поступающих одновременно на вход элемента <math>u_{i}</math><ref name="notsimple" /><ref>Розенблатт Ф., с. 83.</ref>\n* \'\'\'Элементарным перцептроном\'\'\' называется простой перцептрон, у которого \'\'все элементы\xa0— простые\'\'. В этом случае его активизирующая функция имеет вид <math>c_{ij}(t) = U_{i}(t - \\tau)w_{ij}(t)</math><ref name="Розенблатт Ф., с. 93">Розенблатт Ф., с. 93.</ref>.\n\nДополнительно можно указать на следующие концепции, предложенные в книге, и позднее развитые в рамках теории нейронных сетей:\n* \'\'\'[[Перцептрон с перекрёстными связями]]\'\'\'\xa0— это система, в которой существуют связи между элементами одного типа (S, A или R), находящиеся на одинаковом логическом расстоянии от S-элементов, причём все остальные связи\xa0— последовательного типа<ref name="percdef" />.\n* \'\'\'[[Перцептрон с обратной связью]]\'\'\'\xa0— это система, в которой существует хотя бы одна связь от логически более удалённого элемента к менее удалённому<ref name="percdef" />. Согласно современной терминологии такие сети называются [[Рекуррентная нейронная сеть|рекуррентными]].\n* \'\'\'[[Перцептрон с переменными S-A связями]]\'\'\'\xa0— это система, в которой снято ограничение на фиксированные связи от S-элементов к A-элементам. Доказано, что путём оптимизации S—A связей можно добиться значительного улучшения характеристик перцептрона<ref name="s-a-var">Розенблатт, Ф., с. 230.</ref>.\n\n=== Описание на основе предикатов ===\n{{Main|Перцептрон (предикатное описание)}}\nМарвин Минский изучал свойства [[Параллельные вычисления|параллельных вычислений]], частным случаем которых на то время был перцептрон. Для анализа его свойств ему пришлось переизложить теорию перцептронов на язык [[предикат]]ов. Суть подхода заключалась в следующем:<ref group="nb">Изложение в этом разделе несколько упрощено из-за сложности анализа на основе предикатов.</ref><ref>Минский, Пейперт, с. 11—18.</ref>\n* множеству сигналов от S-элементов была сопоставлена переменная X;\n* каждому A-элементу был сопоставлен предикат φ(X) \'\'(фи от икс)\'\', названный \'\'\'частным предикатом\'\'\';\n* каждому R-элементу был сопоставлен предикат ψ \'\'(пси)\'\', зависящий от частных предикатов;\n* наконец, \'\'\'перцептроном\'\'\' было названо устройство, способное вычислять все предикаты типа ψ.\n\nПрименительно к «зрительному» перцептрону, переменная X символизировала образ какой-либо геометрической фигуры (\'\'стимул\'\'). Частный предикат позволял «распознавать» каждый свою фигуру. Предикат ψ означал ситуацию, когда линейная комбинация <math>a_{1}\\phi_{1} + \\ldots + a_{n}\\phi_{n}</math> (<math>a_{i}</math>\xa0— коэффициенты передачи) превышала некоторый порог θ.\n\nУчёные выделили 5 семейств перцептронов, обладающих, по их мнению, интересными свойствами:<ref>Минский, Пейперт, с. 18.</ref>\n# \'\'\'Перцептроны, ограниченные по диаметру\'\'\'\xa0— каждая фигура X, распознаваемая частными предикатами, не превосходит по диаметру некоторую фиксированную величину.\n# \'\'\'Перцептроны ограниченного порядка\'\'\'\xa0— каждый частный предикат зависит от ограниченного количества точек из X.\n# \'\'\'Перцептроны Гамбы\'\'\'\xa0— каждый частный предикат должен быть линейной пороговой функцией, то есть мини-перцептроном.\n# \'\'\'Случайные перцептроны\'\'\'\xa0— перцептроны ограниченного порядка, где частные предикаты представляют собой случайно выбранные булевы функции. В книге отмечается, что именно эта модель наиболее подробно изучалась группой Розенблатта.\n# \'\'\'Ограниченные перцептроны\'\'\'\xa0— множество частных предикатов бесконечно, а множество возможных значений коэффициентов <math>a_{i}</math> конечно.\n\nХотя такой математический аппарат позволил применить анализ только к \'\'\'элементарному перцептрону\'\'\' Розенблатта, он вскрыл много принципиальных ограничений для параллельных вычислений, от которых не свободен ни один вид современных искусственных нейронных сетей.\n\n== Историческая классификация ==\n[[Файл:Neuro.PNG|thumb|350px|Архитектура многослойного перцептрона (обоих подтипов).]]\nПонятие перцептрона имеет интересную, но незавидную историю. В результате неразвитой терминологии нейронных сетей прошлых лет, резкой критики и непонимания задач исследования перцептронов, а иногда и ложного освещения прессой, изначальный смысл этого понятия исказился. Сравнивая разработки Розенблатта и современные обзоры и статьи, можно выделить 4 довольно обособленных класса перцептронов:\n\n; Перцептрон с одним скрытым слоем\n: Это классический перцептрон, которому посвящена бо́льшая часть книги Розенблатта, и рассматриваемый в данной статье: у него имеется по одному слою S-, A- и R-элементов.\n\n; Однослойный перцептрон\n: Это модель, в которой входные элементы напрямую соединены с выходными с помощью системы весов. Является простейшей [[Искусственная нейронная сеть#Сети прямого распространения (Feedforward)|сетью прямого распространения]]\xa0— [[Линейный классификатор|линейным классификатором]], и частным случаем классического перцептрона, в котором каждый S-элемент однозначно соответствует одному A-элементу, S—A связи имеют вес +1 и все A-элементы имеют порог\xa0θ\xa0=\xa01. Однослойные перцептроны фактически являются [[Искусственный нейрон|формальными нейронами]], то есть пороговыми элементами Мак-Каллока\xa0— Питтса. Они имеют множество ограничений, в частности, они не могут идентифицировать ситуацию, когда на их входы поданы разные сигналы («задача XOR», см. [[#Функциональные заблуждения|ниже]]).\n\n; [[Многослойный перцептрон Розенблатта|Многослойный перцептрон]] (по Розенблатту)\n: Это перцептрон, в котором присутствуют дополнительные слои A-элементов. Его анализ провёл Розенблатт в третьей части своей книги.\n\n; [[Многослойный перцептрон Румельхарта|Многослойный перцептрон]] (по Румельхарту)\n: Это перцептрон, в котором присутствуют дополнительные слои A-элементов, причём, обучение такой сети проводится по методу [[Метод обратного распространения ошибки|обратного распространения ошибки]], и обучаемыми являются все слои перцептрона (в том числе S—A). Является частным случаем многослойного перцептрона Розенблатта.\n\nВ настоящее время в литературе под термином «перцептрон» понимается чаще всего однослойный перцептрон ({{lang-en|Single-layer perceptron}}), причём, существует распространённое заблуждение, что именно этот простейший тип моделей предложил Розенблатт. В противоположность однослойному ставят «многослойный перцептрон» ({{lang-en|Multilayer perceptron}}), опять же, чаще всего подразумевая многослойный перцептрон Румельхарта, а не Розенблатта. Классический перцептрон в такой дихотомии относят к многослойным.\n\n== Алгоритмы обучения ==\nВажным свойством любой нейронной сети является [[Машинное обучение|способность к обучению]]. Процесс обучения является процедурой настройки весов и порогов с целью уменьшения разности между желаемыми (целевыми) и получаемыми векторами на выходе. В своей книге Розенблатт пытался классифицировать различные алгоритмы обучения перцептрона, называя их системами подкрепления.\n: \'\'\'Система подкрепления\'\'\'\xa0— это любой набор правил, на основании которых можно изменять с течением времени матрицу взаимодействия (или состояние памяти) перцептрона<ref name="roslearn">Розенблатт, Ф., с. 85—88.</ref>.\nОписывая эти системы подкрепления и уточняя возможные их виды, Розенблатт основывался на идеях [[Хебб, Дональд|Д. Хебба]] об обучении, предложенных им в [[1949 год]]у<ref name="hebb" />, которые можно перефразировать в следующее правило, состоящее из двух частей:\n* Если два нейрона по обе стороны синапса (соединения) активизируются одновременно (то есть синхронно), то прочность этого соединения возрастает.\n* Если два нейрона по обе стороны синапса активизируются асинхронно, то такой синапс ослабляется или вообще отмирает<ref>Хайкин С., 2006, с. 96.</ref>.\n\n=== Обучение с учителем ===\n{{Main|Метод коррекции ошибки}}\nКлассический метод обучения перцептрона\xa0— это \'\'метод коррекции ошибки\'\'<ref name="teorshod" />. Он представляет собой такой вид [[Обучение с учителем|обучения с учителем]], при котором вес связи не изменяется до тех пор, пока текущая реакция перцептрона остаётся правильной. При появлении неправильной реакции вес изменяется на единицу, а знак (+/-) определяется противоположным от знака ошибки.\n\nДопустим, мы хотим обучить перцептрон разделять два класса объектов так, чтобы при предъявлении объектов первого класса выход перцептрона был положителен (+1), а при предъявлении объектов второго класса\xa0— отрицательным (−1). Для этого выполним следующий алгоритм:<ref name="learning" />\n# Случайным образом выбираем пороги для A-элементов и устанавливаем связи S—A (далее они изменяться не будут).\n# Начальные коэффициенты <math>w_i</math> полагаем равными нулю.\n# Предъявляем \'\'\'обучающую выборку\'\'\': объекты (например, круги либо квадраты) с указанием класса, к которым они принадлежат.\n#* Показываем перцептрону объект первого класса. При этом некоторые A-элементы возбудятся. Коэффициенты <math>w_i</math>, соответствующие этим возбуждённым элементам, \'\'увеличиваем\'\' на 1.\n#* Предъявляем объект второго класса и коэффициенты <math>w_i</math> тех A-элементов, которые возбудятся при этом показе, \'\'уменьшаем\'\' на 1.\n# Обе части шага 3 выполним для всей обучающей выборки. В результате обучения сформируются значения весов связей <math>w_i</math>.\n\n\'\'\'[[Теорема сходимости перцептрона]]\'\'\'<ref name="teorshod" />, описанная и доказанная Ф. Розенблаттом (с участием Блока, Джозефа, Кестена и других исследователей, работавших вместе с ним), показывает, что элементарный перцептрон, обучаемый по такому алгоритму, независимо от начального состояния весовых коэффициентов и последовательности появления стимулов \'\'всегда\'\' приведёт к достижению решения за конечный промежуток времени.\n\n=== Обучение без учителя ===\nКроме классического метода обучения перцептрона Розенблатт также ввёл понятие об [[Обучение без учителя|обучении без учителя]], предложив следующий способ обучения:\n: \'\'\'Альфа-система подкрепления\'\'\'\xa0— это система подкрепления, при которой веса всех \'\'активных\'\' связей <math>c_{ij}</math>, которые ведут к элементу <math>u_j</math>, изменяются на одинаковую величину r, а веса \'\'неактивных\'\' связей за это время не изменяются<ref>Розенблатт, Ф., с. 86.</ref>.\nЗатем, с разработкой понятия [[многослойный перцептрон Румельхарта|многослойного перцептрона]], альфа-система была модифицирована и её стали называть \'\'[[дельта-правило]]\'\'. Модификация была проведена с целью сделать функцию обучения [[Производная функции|дифференцируемой]] (например, [[сигмоид]]ной), что в свою очередь нужно для применения метода [[Градиентный спуск|градиентного спуска]], благодаря которому возможно обучение более одного слоя.\n\n=== Метод обратного распространения ошибки ===\n{{Main|Алгоритм обратного распространения ошибки}}\nДля обучения многослойных сетей рядом учёных, в том числе [[Дэвид Румельхарт|Д. Румельхартом]], был предложен [[Градиентный спуск|градиентный алгоритм]] обучения с учителем, проводящий сигнал ошибки, вычисленный \'\'выходами\'\' перцептрона, к его \'\'входам\'\', слой за слоем. Сейчас это самый популярный метод обучения многослойных перцептронов. Его преимущество в том, что он может обучить \'\'все\'\' слои нейронной сети, и его легко просчитать локально. Однако этот метод является очень долгим, к тому же, для его применения нужно, чтобы передаточная функция нейронов была дифференцируемой. При этом в перцептронах пришлось отказаться от бинарного сигнала, и пользоваться на входе [[Вещественное число|непрерывными значениями]]<ref>Хайкин С., 2006, с. 225—243, 304—316.</ref>.\n\n== Традиционные заблуждения ==\nВ результате популяризации искусственных нейронных сетей журналистами и маркетологами был допущен ряд неточностей, которые, при недостаточном изучении оригинальных работ по этой тематике, неверно истолковывались молодыми (на то время) учёными. В результате по сей день можно встретиться с недостаточно глубокой трактовкой функциональных возможностей перцептрона по сравнению с другими нейронными сетями, разработанными в последующие годы.{{когда}}\n\n=== Терминологические неточности ===\nСамая распространённая ошибка, связанная с терминологией, это определение перцептрона как \'\'нейронной сети без скрытых слоёв\'\' (однослойного перцептрона, см. [[#Историческая классификация|выше]]). Эта ошибка связана с недостаточно проработанной терминологией в области нейросетей на раннем этапе их разработки. Ф. Уоссерменом была сделана попытка определённым образом классифицировать различные виды нейронных сетей:\n{{начало цитаты}}\nКак видно из публикаций, нет общепринятого способа подсчёта числа слоёв в сети. Многослойная сеть состоит из чередующихся множеств нейронов и весов. Входной слой не выполняет суммирования. Эти нейроны служат лишь в качестве разветвлений для первого множества весов и не влияют на вычислительные возможности сети. По этой причине первый слой не принимается во внимание при подсчёте слоев, и сеть считается двухслойной, так как только два слоя выполняют вычисления. Далее, веса слоя считаются связанными со следующими за ними нейронами. Следовательно, слой состоит из множества весов со следующими за ними нейронами, суммирующими взвешенные сигналы<ref>\'\'Уоссермен, Ф.\'\' Нейрокомпьютерная техника: Теория и практика, 1992.</ref>.\n{{конец цитаты}}\nВ результате такого представления перцептрон попал под определение «однослойная нейронная сеть». Отчасти это верно, потому что у него нет скрытых слоёв \'\'обучающихся\'\' нейронов (веса которых адаптируются к задаче). И поэтому всю совокупность фиксированных связей системы из S- к A-элементам, можно логически заменить набором (модифицированных по жёсткому правилу) \'\'новых\'\' входных сигналов, поступающих сразу на А-элементы (устранив тем самым вообще первый слой связей). Но тут как раз не учитывают, что такая модификация превращает нелинейное представление задачи в линейное.\n\nПоэтому просто игнорирование не обучаемых слоёв с фиксированными связями (в элементарном перцептроне это S—A связи) позволяет делать неправильные выводы о возможностях нейросети. Так, Минский поступил очень корректно, переформулировав А-элемент как [[предикат]] (то есть функцию); наоборот, Уоссермен уже потерял такое представление и у него А-элемент\xa0— просто вход (почти эквивалентный S-элементу). При такой терминологической путанице упускается из виду тот факт, что в перцептроне происходит [[отображение]] [[рецептивное поле|рецептивного поля]] S-элементов на ассоциативное поле А-элементов, в результате чего и происходит преобразование любой линейно неразделимой задачи в линейно разделимую.\n\n=== Функциональные заблуждения ===\n[[Файл:Perceptron XOR task v2.svg|300px|thumb|Решение элементарным перцептроном «задачи XOR». Порог всех элементов θ = 0.]]\nБольшинство функциональных заблуждений сводятся к якобы невозможности решения перцептроном нелинейно разделяемой задачи. Но вариаций на эту тему достаточно много, рассмотрим главные из них.\n\n==== Задача XOR ====\n\'\'Перцептрон не способен решить «[[исключающее или|задачу XOR]]».\'\'\n: Очень распространённое и самое несерьёзное заявление. На изображении справа показано решение этой задачи перцептроном. Данное заблуждение возникает, во-первых, из-за того, что неправильно интерпретируют определение перцептрона, данного Минским (см. [[#Модель на основе предикатов|выше]]), а именно, предикаты сразу приравнивают входам, хотя предикат у Минского\xa0— это функция, идентифицирующая целый набор входных значений<ref group="nb">Предикат эквивалентен входу лишь в частном случае\xa0— только когда он зависит от одного аргумента.</ref>. Во-вторых, из-за того, что классический перцептрон Розенблатта путают с однослойным перцептроном (из-за терминологической неточности, описанной выше).\nСледует обратить особое внимание на то, что «однослойный перцептрон» в современной терминологии и «однослойный перцептрон» в терминологии Уоссермана\xa0— разные объекты. И объект, изображённый на иллюстрации, в терминологии Уоссермана есть двухслойный перцептрон.\n\n==== Обучаемость линейно неразделимым задачам ====\n\'\'Выбором случайных весов \'\'\'можно\'\'\' достигнуть обучения и линейно неразделимым (вообще, любым) задачам, \'\'\'но только если повезёт\'\'\', и в новых переменных (выходах A-нейронов) задача окажется линейно разделимой. Но может и не повезти.\'\'\n\n: [[Теорема сходимости перцептрона]]<ref name="teorshod" /> доказывает, что нет и не может быть никакого «может и не повезти»; при равенстве А-элементов числу стимулов и не особенной [[G-матрица перцептрона|G-матрице]]\xa0— вероятность решения равна 100%. То есть при [[отображение|отображении]] рецепторного поля на ассоциативное поле большей на одну [[Размерность пространства|размерности]] случайным (нелинейным) [[Оператор (математика)|оператором]] нелинейная задача превращается в линейно разделимую. А следующий обучаемый слой уже находит линейное решение в другом пространстве входов.\n\n: Например, обучение перцептрона для решения «задачи XOR» (см. на иллюстрации) проводится следующими этапами:\n<div align="center">\n{| class="wikitable"\n!rowspan="2"|Веса\n!colspan="9"|Итерации\n|-\n!colspan="3"|1\n!2\n!colspan="2"|3\n!colspan="2"|4\n!5\n|-\n!w1\n| width="40" | 0\n| width="40" | 1\n| width="40" | 1\n| width="40" | 1\n| width="40" | 1\n| width="40" | 2\n| width="40" | 2\n| width="40" | 2\n| width="40" | 2\n|-\n!w2\n|  0|| 0|| 1|| 1|| 1||  1||  1|| 2||  2\n|-\n!w3\n| −1|| 0|| 1|| 0|| −1|| 0|| −1|| 0|| −1\n|-\n!Входные сигналы (x, y)\n|  1, 1|| 0, 1|| 1, 0|| 1, 1||  1, 1|| 0, 1||  1, 1|| 1, 0||  1, 1\n|}</div>\n\n==== Обучаемость на малом числе примеров ====\n\'\'Если в задаче размерность входов довольно высока, а обучающих примеров мало, то в таком «слабо заполненном» пространстве число удач может и не оказаться малым. Это свидетельствует лишь о частном случае пригодности перцептрона, а не его универсальности.\'\'\n\n: Данный аргумент легко проверить на тестовой задаче под названием «шахматная доска» или «губка с водой»<ref>Бонгард, М. М., с. 29.</ref><ref group="nb">[[Бонгард, Михаил Моисеевич|М.\xa0М.\xa0Бонгард]] считает эту задачу наисложнейшей для проведения гиперплоскости в пространстве рецепторов.</ref>:\n{{Задача|Дана цепочка из 2·\'\'N\'\' единиц или нулей, параллельно поступающих на входы перцептрона. Если эта цепочка является зеркально симметричной относительно центра, то на выходе +1, иначе 0. Обучающие примеры — \'\'все\'\' (это важно) <math>2^{2N}</math> цепочек.}}\n: Могут быть вариации данной задачи, например:\n{{Задача|Возьмём [[битовое изображение|чёрно-белое изображение]] размером 256×256 элементов ([[пиксел]]ов). Входными данными для перцептрона будут координаты точки (8 бит + 8 бит, итого нужно 16 S-элементов), на выходе потребуем цвет точки. Обучаем перцептрон всем точкам (всему изображению). В итоге имеем 65&nbsp;536 различных пар «стимул—реакция». Обучить без ошибок.}}\n: Если данный аргумент справедлив, то перцептрон не сможет ни при каких условиях обучиться, не делая ни одной ошибки. Иначе перцептрон не ошибётся ни разу.\n\n: На практике оказывается, что данная задача очень проста для перцептрона: чтобы её решить, перцептрону достаточно 1500 А-элементов (вместо полных 65\xa0536, необходимых для любой задачи). При этом число [[Итерация|итераций]] порядка 1000. При 1000 А-элементов перцептрон не сходится за 10\xa0000 итераций. Если же увеличить число А-элементов до 40\xa0000, то схождения можно ожидать за 30—80 итераций.\n\n: Такой аргумент появляется из-за того, что данную задачу путают с задачей Минского «о предикате „чётность“»<ref name="chetnost">Минский М., Пейперт С., с. 59.</ref>.\n\n==== Стабилизация весов и сходимость ====\n\'\'В перцептроне Розенблатта столько А-элементов, сколько входов. И сходимость по Розенблатту, это стабилизация весов.\'\'\n\n: У Розенблатта читаем:\n{{начало цитаты}}\nЕсли число стимулов в пространстве W равно n > N (то есть больше числа А-элементов элементарного перцептрона), то существует некоторая [[классификация]] {{nobr|С(W)}}, для которой решения не существует<ref>Розенблатт, Ф., с. 101.</ref>.\n{{конец цитаты}}\n: Отсюда следует, что:\n:# у Розенблатта число А-элементов равно числу стимулов (обучающих примеров), а не числу входов;\n:# сходимость по Розенблатту, это не стабилизация весов, а наличие всех требуемых классификаций, то есть по сути отсутствие ошибок.\n\n==== Экспоненциальный рост числа скрытых элементов ====\n\'\'Если весовые коэффициенты к элементам скрытого слоя (А-элементам) фиксированы, то необходимо, чтобы количество элементов скрытого слоя (либо их сложность) экспоненциально возрастало с ростом размерности задачи (числа рецепторов). Тем самым, теряется их основное преимущество\xa0— способность решать задачи произвольной сложности при помощи простых элементов.\'\'\n\n: Розенблаттом было показано, что число А-элементов зависит только от числа стимулов, которые нужно распознать (см. предыдущий пункт или [[теорема сходимости перцептрона|теорему сходимости перцептрона]]). Таким образом, при возрастании числа рецепторов, если количество А-элементов фиксировано, непосредственно не зависит возможность перцептрона к решению задач произвольной сложности.\n\n: Такое заблуждение происходит от следующей фразы Минского:\n{{начало цитаты}}\nПри исследовании предиката «чётность» мы видели, что коэффициенты могут расти с ростом |R| (числа точек на изображении) экспоненциально<ref>Минский, Пейперт, с. 155, 189 (недословно, упрощённо для выразительности).</ref>.\n{{конец цитаты}}\n: Кроме того, Минский исследовал и другие предикаты, например «равенство». Но все эти предикаты представляют собой достаточно специфическую задачу на обобщение, а не на распознавание или прогнозирование. Так, например, чтобы перцептрон мог выполнять предикат «четность»\xa0— он должен сказать, чётно или нет число чёрных точек на чёрно-белом изображении; а для выполнения предиката «равенство»\xa0— сказать, равна ли правая часть изображения левой. Ясно, что такие задачи выходят за рамки задач распознавания и прогнозирования, и представляют собой задачи на обобщение или просто на подсчёт определённых характеристик. Это и было убедительно показано Минским, и является ограничением не только перцептронов, но и всех [[Параллельные вычисления|параллельных алгоритмов]], которые не способны быстрее последовательных алгоритмов вычислить такие предикаты.\n\n: Поэтому такие задачи ограничивают возможности всех нейронных сетей и перцептронов в частности, но это никак не связанно с фиксированными связями первого слоя; так как во-первых, речь шла о величине коэффициентов связей второго слоя, а во-вторых, вопрос только в эффективности, а не принципиальной возможности. То есть перцептрон можно обучить и этой задаче, но требуемые для этого ёмкость памяти и скорость обучения будут больше, чем при применении простого последовательного алгоритма. Введение же обучаемых весовых коэффициентов в первом слое лишь ухудшит положение дел, ибо потребует большего времени обучения, потому что переменные связи между S и A скорее препятствуют, чем способствуют процессу обучения<ref>Розенблатт, стр. 239</ref>. Причём, при подготовке перцептрона к задаче распознавания стимулов особого типа, для сохранения эффективности потребуются особые условия стохастического обучения<ref name=autogenerated1>Розенблатт, стр. 242</ref>, что было показано Розенблаттом в экспериментах с [[Перцептрон с переменными S-A связями|перцептроном с переменными S—A связями]].\n\n== Возможности и ограничения модели ==\n{{Main|Возможности и ограничения перцептронов}}\n\n=== Возможности модели ===\n[[Файл:Edge approximation.svg|thumb|200px|left|Пример [[Классификация|классификации]] объектов. Зелёная линия\xa0— граница классов.]]\nСам Розенблатт рассматривал перцептрон прежде всего как следующий важный шаг в сторону исследования и использования нейронных сетей, а не как оконченный вариант «[[Может ли машина мыслить?|машины, способной мыслить]]»<ref group="nb">На первых этапах развития науки об искусственном интеллекте её задача рассматривалась в абстрактном смысле\xa0— создание систем, напоминающих по разуму человека (см. [[искусственный общий интеллект]]). Современные формулировки задач в ИИ, как правило, более аккуратны.</ref>. Ещё в предисловии к своей книге он, отвечая на критику, отмечал, что «программа по исследованию перцептрона связана главным образом \'\'не\'\' с изобретением устройств, обладающих „искусственным интеллектом“, а с изучением физических структур и нейродинамических принципов»<ref>Розенблатт, Ф., с. 18.</ref>.\n\nРозенблатт предложил ряд [[Психологический тест|психологических тестов]] для определения возможностей нейросетей: эксперименты по [[Кластеризация|различению]], [[Обобщение понятий|обобщению]], по [[Поиск закономерностей|распознаванию последовательностей]], [[Абстрагирование|образованию абстрактных понятий]], формированию и свойствам «[[Самосознание|самосознания]]», [[Творчество|творческого]] [[Воображение|воображения]] и другие<ref>Розенблатт, Ф., с. 70—77.</ref>. Некоторые из этих экспериментов далеки от современных возможностей перцептронов, поэтому их развитие происходит больше [[Философия|философски]] в пределах направления [[коннективизм]]а. Тем не менее, для перцептронов установлены два важных факта, находящие применение в практических задачах: \'\'возможность [[классификация|классификации]]\'\' (объектов) и \'\'возможность [[аппроксимация|аппроксимации]]\'\' (границ классов и функций)<ref name="ezhov">см. \'\'Ежов А. А., Шумский С. А.\'\' «Нейрокомпьютинг…», 2006. [http://www.intuit.ru/department/expert/neurocomputing/3/2.html Лекция 3: Обучение с учителем: Распознавание образов]</ref>.\n\nВажным свойством перцептронов является их способность к обучению, причём по довольно простому и эффективному алгоритму (см. [[#Алгоритмы обучения|выше]]).\n\n=== Ограничения модели ===\n\n{{main|Перцептроны (книга)}}\n\n[[Файл:Invariant image recognition.svg|thumb|180px|Некоторые задачи, которые перцептрон не способен решить: 1, 2\xa0— преобразования группы переносов; 3\xa0— из какого количества частей состоит фигура? 4\xa0— внутри какого объекта нет другой фигуры? 5\xa0— какая фигура внутри объектов повторяется два раза? (3, 4, 5\xa0— задачи на определение «связности» фигур.)]]\n\nСам Розенблатт выделил два [[Фундаментальные ограничения|фундаментальных ограничения]] для трёхслойных перцептронов (состоящих из одного S-слоя, одного A-слоя и R-слоя): отсутствие у них способности к обобщению своих характеристик на новые стимулы или новые ситуации, а также неспособность анализировать сложные ситуации во внешней среде путём расчленения их на более простые<ref name="Розенблатт Ф., с. 93"/>.\n\nВ [[1969 год]]у [[Минский, Марвин Ли|Марвин Минский]] и [[Паперт, Сеймур|Сеймур Паперт]] опубликовали книгу «Перцептроны», где математически показали, что перцептроны, подобные розенблаттовским, принципиально не в состоянии выполнять многие из тех функций, которые хотели получить от перцептронов. К тому же, в то время была слабо развита теория о параллельных вычислениях, а перцептрон полностью соответствовал принципам таких вычислений. По большому счёту, Минский показал преимущество последовательных вычислений перед [[Параллельные вычисления|параллельным]] в определённых классах задач, связанных с инвариантным представлением. Его критику можно разделить на три темы:\n\n# Перцептроны имеют ограничения в задачах, связанных с \'\'инвариантным представлением\'\' образов, то есть независимым от их положения на сенсорном поле и относительно других фигур. Такие задачи возникают, например, если нам требуется построить машину для чтения [[Символ|печатных букв или цифр]] так, чтобы эта машина могла распознавать их независимо от положения на странице (то есть чтобы на решение машины не оказывали влияния [[Параллельный перенос|перенос]], [[поворот]], [[растяжение-сжатие]] символов)<ref name="invariant" />; или если нам нужно определить из скольких частей состоит фигура<ref>Минский М., Пейперт С., с. 76—98.</ref>; или находятся ли две фигуры рядом или нет<ref>Минский М., Пейперт С., с. 113—116.</ref>. Минским было доказано, что этот тип задач невозможно полноценно решить с помощью параллельных вычислений, в том числе\xa0— перцептрона.\n# Перцептроны не имеют функционального преимущества над [[Аналитические методы|аналитическими методами]] (например, [[Статистические методы|статистическими]]) в задачах, связанных с [[прогнозирование]]м<ref>Минский М., Пейперт С., с. 192—214.</ref>. Тем не менее, в некоторых случаях они представляют более простой и [[Производительность|производительный]] метод [[Интеллектуальный анализ данных|анализа данных]].\n# Было показано, что некоторые задачи в принципе могут быть решены перцептроном, но могут потребовать нереально [[Теория сложности вычислений|большого времени]]<ref>Минский, Пейперт, с. 163—187</ref> или нереально большой [[Оперативная память|памяти]]<ref>Минский, Пейперт, с. 153—162</ref>.\n\nКнига Минского и Паперта существенно повлияла на пути развития науки об искусственном интеллекте, так как переместила научный интерес и субсидии правительственных организаций [[США]] на другое направление исследований\xa0— [[Искусственный интеллект|символьный подход в ИИ]].\n\n=== Применение перцептронов ===\nЗдесь будут показаны только основы практического применения перцептрона на двух различных задачах. Задача \'\'[[Прогнозирование|прогнозирования]]\'\' (и эквивалентная ей задача \'\'[[Распознавание образов|распознавания образов]]\'\') требует высокой точности, а задача \'\'[[интеллектуальный агент|управления агентами]]\'\'\xa0— высокой скорости обучения. Поэтому, рассматривая эти задачи, можно полноценно ознакомиться с возможностями перцептрона, однако этим далеко не исчерпываются варианты его использования.\n\nВ практических задачах от перцептрона потребуется возможность выбора более чем из двух вариантов, а значит, на выходе у него должно находиться более одного R-элемента. Как показано Розенблаттом, характеристики таких систем не отличаются существенно от характеристик элементарного перцептрона<ref>Розенблатт, Ф., с. 219—224.</ref>.\n\n==== Прогнозирование и распознавание образов ====\n[[Файл:Learning-agent-ru.svg|thumb|350px|right|Взаимодействие [[Интеллектуальный агент|обучающегося агента]] со средой. Важной частью такой системы являются обратные связи.]]\nВ этих задачах от перцептрона требуется установить принадлежность объекта к какому-либо классу по его параметрам (например, по внешнему виду, форме, силуэту). Причём, точность распознавания будет во многом зависеть от представления выходных реакций перцептрона. Здесь возможны три типа кодирования: [[Конфигурационное кодирование|конфигурационное]], [[Позиционное кодирование|позиционное]], и гибридное. Позиционное кодирование, когда каждому классу соответствует свой R-элемент, даёт более точные результаты, чем другие виды. Такой тип использован, например, в работе Э. Куссуль и др. «Перцептроны Розенблатта для распознавания рукописных цифр». Однако оно неприменимо в тех случаях, когда число классов значительно, например, несколько сотен. В таких случаях можно применять гибридное конфигурационно-позиционное кодирование, как это было сделано в работе С. Яковлева «Система распознавания движущихся объектов на базе искусственных нейронных сетей».\n\n==== Управление агентами ====\nВ искусственном интеллекте часто рассматриваются обучающиеся ([[Адаптивная система|адаптирующиеся]] к [[окружающая среда|окружающей среде]]) агенты. При этом в [[условия неопределённости|условиях неопределённости]] становится важным анализировать не только текущую информацию, но и общий контекст ситуации, в которую попал агент, поэтому здесь применяются [[перцептроны с обратной связью]]<ref>\'\'Яковлев С. С.\'\' [http://ru.vlab.wikia.com/wiki/Сергей_Яковлев:Статья:Yak1 Использование принципа рекуррентности Джордана в перцептроне Розенблатта, Журнал «АВТОМАТИКА И ВЫЧИСЛИТЕЛЬНАЯ ТЕХНИКА», Рига, 2009]. Virtual Laboratory Wiki.</ref>. Кроме того, в некоторых задачах становится важным повышение скорости обучения перцептрона, например, с помощью моделирования [[Рефрактерный период|рефрактерности]]<ref>\'\'Яковлев С. С.\'\', Investigation of Refractoriness principle in Recurrent Neural Networks, Scientific proceedings of Riga Technical University, Issue 5, Vol.36, RTU, Riga, 2008, P. 41-48. [http://ru.vlab.wikia.com/wiki/Сергей_Яковлев:Статья:Yak2 Исследование принципа рефрактерности в рекуррентных нейронных сетях (перевод) ],</ref>.\n\nПосле периода, известного как «[[Зима искусственного интеллекта]]», интерес к кибернетическим моделям возродился в [[1980-е|1980-х годах]], так как сторонники символьного подхода в ИИ так и не смогли подобраться к решению вопросов о «Понимании» и «Значении», из-за чего [[машинный перевод]] и техническое распознавание образов до сих пор обладает неустранимыми недостатками. Сам Минский публично выразил сожаление, что его выступление нанесло урон концепции перцептронов, хотя книга лишь показывала недостатки отдельно взятого устройства и некоторых его вариаций. Но в основном ИИ стал синонимом символьного подхода, который выражался в составлении все более сложных программ для компьютеров, моделирующих сложную деятельность человеческого мозга.\n\n== См. также ==\n* [[Свёрточная нейронная сеть]]\n* [[Биокомпьютинг]]\n* [[Байесовская сеть доверия]]\n* [[Когнитрон]]\n* [[История искусственного интеллекта]]\n* [[Паттерн (психология)]]\n\n== Примечания ==\n{{примечания|group=nb}}\n\n== Источники ==\n{{примечания|2}}\n\n== Литература ==\n* {{книга\n|автор          = [[Бонгард, Михаил Моисеевич|Бонгард, М. М.]]\n|заглавие       = Проблема узнавания\n|оригинал       = \n|ссылка         = http://vlabdownload.googlecode.com/files/Bongard.djvu\n|место          = М.\n|издательство   = Наука\n|год            = 1967\n|страниц        = 320\n}}\n* {{книга\n|автор          = Брюхомицкий, Ю. А.\n|заглавие       = Нейросетевые модели для систем информационной безопасности: Учебное пособие\n|ссылка         = http://window.edu.ru/window_catalog/redir?id=28836&file=tsure062.pdf\n|место          = Таганрог\n|издательство   = Изд-во ТРТУ\n|год            = 2005\n|страниц        = 160\n}}\n* {{статья\n| автор         = [[Маккалок, Уоррен|Мак-Каллок, У. С.]], [[Питтс, Уоррен|Питтс, В.]]\n| заглавие      = Логическое исчисление идей, относящихся к нервной активности\n| оригинал      = A logical calculus of the ideas immanent in nervous activity\n| ссылка        = http://vlabdownload.googlecode.com/files/mcculloch.pdf\n| издание       = Автоматы\n| тип           = Сб.\n| место         = М.\n| год           = 1956\n| страницы      = 363—384\n}}\n* {{книга\n|автор          = [[Минский, Марвин Ли|Минский, М.]], [[Паперт, Сеймур|Пейперт, С.]]\n|заглавие       = Персептроны\n|оригинал       = Perceptrons\n|ссылка         = http://vlabdownload.googlecode.com/files/perceptrons2.djvu\n|место          = М.\n|издательство   = Мир\n|год            = 1971\n|страниц        = 261\n}}\n* {{книга\n|автор          = [[Розенблатт, Фрэнк|Розенблатт, Ф.]]\n|заглавие       = Принципы нейродинамики: Перцептроны и теория механизмов мозга\n|оригинал       = Principles of Neurodynamic: Perceptrons and the Theory of Brain Mechanisms\n|ссылка         = http://vlabdownload.googlecode.com/files/ros_principles2.djvu\n|место          = М.\n|издательство   = Мир\n|год            = 1965\n|страниц        = 480\n}}\n* {{книга\n|автор          = Уоссермен, Ф.\n|заглавие       = Нейрокомпьютерная техника: Теория и практика\n|оригинал       = Neural Computing. Theory and Practice\n|ссылка         = http://evrika.tsi.lv/index.php?name=texts&file=show&f=410\n|место          = М.\n|издательство   = Мир\n|год            = 1992\n|страниц        = 240\n|isbn           = 5-03-002115-9\n}}\n* {{книга\n|автор          = Хайкин, С.\n|заглавие       = Нейронные сети: Полный курс\n|оригинал       = Neural Networks: A Comprehensive Foundation\n|ссылка         = \n|издание        = 2-е изд\n|место          = М.\n|издательство   = [[Вильямс (издательство)|«Вильямс»]]\n|год            = 2006\n|страниц        = 1104\n|isbn           = 0-13-273350-1\n}}\n* {{статья\n| автор         = Яковлев С. С.\n| заглавие      = Система распознавания движущихся объектов на базе искусственных нейронных сетей\n| ссылка        = http://ru.vlab.wikia.com/wiki/Сергей_Яковлев:Статья:RecognitionDynamic\n| издание       = ИТК НАНБ\n| место         = Минск\n| год           = 2004\n| страницы      = 230—234\n}}\n* {{статья\n| автор         = Kussul E., Baidyk T., Kasatkina L., Lukovich V.\n| оригинал      = Rosenblatt Perceptrons for Handwritten Digit Recognition\n| заглавие      = Перцептроны Розенблатта для распознавания рукописных цифр\n| ссылка        = http://vlabdownload.googlecode.com/files/IJCNN2001.pdf\n| издание       = IEEE\n| год           = 2001\n| страницы      = 1516—1520\n| ISBN          = 0-7803-7044-9\n}} {{ref-en}}\n* {{статья\n| автор         = Stormo G. D., Schneider T. D., Gold L., Ehrenfeucht A.\n| оригинал      = Use of the \'Perceptron\' algorithm to distinguish transational initiation sites in E. coli\n| заглавие      = Использование перцептрона для выделения сайтов инициации в E. coli\n| ссылка        = https://www.ncbi.nlm.nih.gov/pmc/articles/PMC320670/pdf/nar00378-0235.pdf\n| издание       = Nucleic Acids Research\n| год           = 1982\n| страницы      = P. 2997–3011\n| ISBN          = \n}} {{ref-en}}\n\n== Ссылки ==\n* {{cite web\n | url         = http://ru.vlab.wikia.com/wiki/Перцептрон\n | title       = Перцептрон\n | work        = \n | publisher   = Virtual Laboratory Wiki\n | accessdate  = 2009-01-17\n | lang        = \n | description = \n | archiveurl  = https://www.webcitation.org/613rARwE9\n | archivedate = 2011-08-19\n}}\n* {{cite web\n | url         = http://www.algoritmy.info/intellect5.html\n | title       = Появление перцептрона\n | accessdate  = 2009-01-17\n | archiveurl  = https://www.webcitation.org/613rBMFrU\n | archivedate = 2011-08-19\n}}\n* {{cite web\n | author        = Ежов А. А., Шумский С. А.\n | authorlink    = \n | datepublished = 2006\n | url           = http://www.intuit.ru/department/expert/neurocomputing/\n | title         = Нейрокомпьютинг и его применения в экономике и бизнесе\n | format        = \n | work          = \n | publisher     = ИНТУИТ\n | accessdate    = 2009-01-17\n | lang          = \n | description   = \n | archiveurl    = https://www.webcitation.org/613rBmf0H\n | archivedate   = 2011-08-19\n}}\n* {{cite web\n | author        = Редько В. Г.\n | datepublished = 1999\n | url           = http://www.keldysh.ru/pages/BioCyber/Lectures/Lecture11/Lecture11.html\n | title         = Искусственные нейронные сети\n | accessdate    = 2009-01-17\n | archiveurl    = https://www.webcitation.org/613rDiAfE\n | archivedate   = 2011-08-19\n}}\n* {{cite web\n | author        = Яковлев С. С.\n | datepublished = 2006\n | url           = http://www.tachome.times.lv/Articles/Linejnost&Invariantnost_2006/Invariantnost.pdf\n | title         = Линейность и инвариантность в искусственных нейронных сетях\n | format        = pdf\n | accessdate    = 2009-01-17\n | archiveurl    = https://www.webcitation.org/613rEbDlz\n | archivedate   = 2011-08-19\n}}\n* {{cite web\n | author        = Estebon, M. D.; Tech, V.\n | datepublished = 1997\n | url           = http://ei.cs.vt.edu/~history/Perceptrons.Estebon.html\n | title         = Perceptrons: An Associative Learning Network\n | lang          = en\n | accessdate    = 2009-01-17\n | archiveurl    = https://www.webcitation.org/613rF8fxO\n | archivedate   = 2011-08-19\n}}\n* {{cite web\n | author        = Беркинблит М. Б.\n | datepublished = 1993\n | url           = http://www.katenke.net/static/berkinblit/perseptron.html\n | title         = Нейронные сети. Глава "Перцептроны и другие обучающиеся классификационные системы"\n | accessdate    = 2009-01-17\n | archiveurl    = https://www.webcitation.org/613rFYD1N\n | archivedate   = 2011-08-19\n}}\n\n{{Нейросети}}\n\n{{Избранная статья|Компьютер|Математика}}\n\n[[Категория:Перцептрон| ]]\n[[Категория:Аналоговые компьютеры]]'}]}}}}